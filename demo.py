#!/usr/bin/env python3
"""
demoCrawler.py

ë°œí‘œ/ì‹œì—°ìš© ë°ëª¨ í¬ë¡¤ëŸ¬
- ë¯¸ë¦¬ ì„ ì •í•œ "ê²Œì‹œê¸€ ìƒì„¸ URL"ë§Œ í¬ë¡¤ë§
- ì œëª©ì„ ì¶”ì¶œí•´ì„œ ì•ˆë“œë¡œì´ë“œ ì„œë²„ë¡œ ì „ì†¡(process_page)ë§Œ ìˆ˜í–‰
"""

import sys
import requests
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env")

# crawler ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€ (sendToServer.py ë“±)
sys.path.insert(0, str(Path(__file__).parent))

from sendToServer import process_page

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ğŸ“ ì‹œì—°ì— ì‚¬ìš©í•  "ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ URL"ë“¤ì„ ì—¬ê¸°ì— ë„£ì–´ì„œ ì‚¬ìš©
DEMO_ARTICLE_URLS = [
    # ì˜ˆì‹œ) í•™ì‚¬ ê³µì§€ ëª‡ ê°œ
    "https://www.kumoh.ac.kr/ko/sub06_01_01_01.do?mode=view&articleNo=545717&article.offset=0&articleLimit=10",
    "https://www.kumoh.ac.kr/ko/sub06_01_01_01.do?mode=view&articleNo=534374&article.offset=0&articleLimit=10",
    "https://www.kumoh.ac.kr/ko/sub06_01_01_01.do?mode=view&articleNo=430818&article.offset=90&articleLimit=10",
]


class DemoCrawler:
    """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•˜ëŠ” ë°ëª¨ ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, article_urls: list[str], dry_run: bool = False):
        """
        Args:
            article_urls: ì‹œì—°ì— ì‚¬ìš©í•  ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ URL ëª©ë¡
            dry_run: True ì´ë©´ ì•ˆë“œë¡œì´ë“œ ì„œë²„ë¡œ ì „ì†¡í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ì¶œë ¥
        """
        self.article_urls = article_urls
        self.dry_run = dry_run
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "sent": 0,   # ì•ˆë“œë¡œì´ë“œ ì„œë²„ ì „ì†¡ ì„±ê³µ ìˆ˜
        }

    def _extract_board_title(self, html: str) -> str | None:
        """
        ê¸ˆì˜¤ ê²Œì‹œíŒ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©ë§Œ ì¶”ì¶œ
        (ê¸°ì¡´ SimpleTestCrawler._extract_board_title ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)
        """
        soup = BeautifulSoup(html, "html.parser")
        head = soup.find("div", class_="title-area")
        if not head:
            return None

        for tag in ["h4", "h3", "strong"]:
            el = head.find(tag)
            if el:
                text = el.get_text(strip=True)
                if text:
                    return text

        return None

    def crawl_detail_page(self, url: str) -> bool:
        """
        ë‹¨ì¼ ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•´ì„œ
        ì œëª©ì„ ì¶”ì¶œí•˜ê³  ì•ˆë“œë¡œì´ë“œ ì„œë²„ë¡œ ì „ë‹¬
        """
        self.stats["total"] += 1
        logger.info(f"ğŸ“„ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            headers = {
                "User-Agent": "KITBot-Demo/1.0 (CSEcapstone, contact: cdh5113@naver.com)"
            }
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()

            html = resp.text
            title = self._extract_board_title(html)

            if not title:
                # fallback: <title> íƒœê·¸ ë“±ì—ì„œë¼ë„ ê°€ì ¸ì˜¤ê¸°
                soup = BeautifulSoup(html, "html.parser")
                if soup.title:
                    title = soup.title.get_text(strip=True)
                else:
                    title = "(ì œëª© ì¶”ì¶œ ì‹¤íŒ¨)"

            logger.info(f"   âœ… ì¶”ì¶œëœ ì œëª©: {title}")

            # ì‹¤ì œ ì‹œì—°ìš© ì „ì†¡
            if self.dry_run:
                logger.info("   [DRY-RUN] process_page í˜¸ì¶œ ìƒëµ (ì‹œì—° í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
            else:
                try:
                    process_page(
                        url=url,
                        title=title,
                    )
                    self.stats["sent"] += 1
                    logger.info("   ğŸ“¡ ì•ˆë“œë¡œì´ë“œ ì„œë²„ë¡œ ë©”íƒ€ë°ì´í„° ì „ì†¡ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"   âš ï¸ ì•ˆë“œë¡œì´ë“œ ì„œë²„ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")

            self.stats["success"] += 1
            return True

        except requests.RequestException as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False

    def run(self):
        """ë°ëª¨ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("=" * 80)
        print("ğŸ¬ ë°ëª¨ í¬ë¡¤ëŸ¬ ì‹œì‘ (ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì „ìš©)")
        print("=" * 80)
        print(f"ëŒ€ìƒ ê²Œì‹œê¸€ ìˆ˜: {len(self.article_urls)}")
        print(f"DRY-RUN ëª¨ë“œ: {'ON (ì„œë²„ ì „ì†¡ X)' if self.dry_run else 'OFF (ì‹¤ì œ ì „ì†¡)'}")
        print("=" * 80)

        start_time = datetime.now()

        for idx, url in enumerate(self.article_urls, 1):
            print(f"\n[{idx}/{len(self.article_urls)}] {url}")
            print("-" * 80)
            self.crawl_detail_page(url)

        elapsed = datetime.now() - start_time

        print("\n" + "=" * 80)
        print("ë°ëª¨ í¬ë¡¤ë§ ì™„ë£Œ")
        print("=" * 80)
        print(f"ì´ ì‹œë„:   {self.stats['total']}")
        print(f"ì„±ê³µ:      {self.stats['success']}")
        print(f"ì‹¤íŒ¨:      {self.stats['failed']}")
        print(f"ì„œë²„ ì „ì†¡: {self.stats['sent']} (dry-run ì´ë©´ í•­ìƒ 0)")
        print(f"\nì†Œìš” ì‹œê°„: {elapsed}")
        print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ë°ëª¨ í¬ë¡¤ëŸ¬ - ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì „ìš©")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì•ˆë“œë¡œì´ë“œ ì„œë²„ë¡œ ì „ì†¡í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ì¶œë ¥",
    )
    args = parser.parse_args()

    if not DEMO_ARTICLE_URLS:
        print("âš ï¸ DEMO_ARTICLE_URLS ì— ì‹œì—°ì— ì‚¬ìš©í•  ê²Œì‹œê¸€ URLì„ ë¨¼ì € ì±„ì›Œì£¼ì„¸ìš”.")
        return

    crawler = DemoCrawler(article_urls=DEMO_ARTICLE_URLS, dry_run=args.dry_run)
    crawler.run()


if __name__ == "__main__":
    main()
