#!/usr/bin/env python3
"""
departmentCrawler.py

í•™ê³¼ ì†Œê°œ / ë™ì•„ë¦¬ ì†Œê°œ / êµìœ¡ê³¼ì •(ì •ì  í˜ì´ì§€ ìœ„ì£¼) 1íšŒì„± í¬ë¡¤ëŸ¬
- ìì£¼ ë³€í•˜ì§€ ì•ŠëŠ” ì •ì  ì •ë³´ìš©
- ê¸°ì¡´ SimpleTestCrawler ë¡œì§ì„ ë³µë¶™/ê²½ëŸ‰í™”í•œ ë²„ì „
"""

import sys
import requests
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import logging
import hashlib
import urllib.parse

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env")

# crawler ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from filters.content_extractor import ContentExtractor
from filters.quality_filter import QualityFilter
from storage.json_storage import JSONStorage
from storage.minio_storage import MinIOStorage
from sendToServer import process_page

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

exclude_patterns = [
    "/cms/fileDownload.do",
]
# í˜ì´ì§€ í¬ë¡¬(ë¡œê³ , ë©”ë‰´, SNS, ë²„íŠ¼ ë“±) ì´ë¯¸ì§€ í•„í„°
ICON_IMAGE_KEYWORDS = [
    "/_res/ko/img/icon/",
    "/_res/ko/img/common/",
    "logo",
    "btn_",
    "btn-",
    "bg_subvisual",
    "wa-mark",
    "bubble_tail",
    "btn_top_go",
]

class departmentCrawler:
    """í•™ê³¼/ë™ì•„ë¦¬/ì •ì  ì†Œê°œ í˜ì´ì§€ ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, enable_minio: bool = False):
        """
        Args:
            enable_minio: MinIO ì‚¬ìš© ì—¬ë¶€ (Trueë©´ ì²¨ë¶€íŒŒì¼ì„ MinIOì— ì—…ë¡œë“œ)
        """
        # MinIO ì„¤ì •
        self.enable_minio = enable_minio
        if enable_minio:
            try:
                self.minio = MinIOStorage.from_env()
                logger.info("âœ… MinIO ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  MinIO ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                logger.warning("   ì²¨ë¶€íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡ë©ë‹ˆë‹¤.")
                self.enable_minio = False
                self.minio = None
        else:
            self.minio = None

        # âœ… ì—¬ê¸° ë¦¬ìŠ¤íŠ¸ì— í•™ê³¼/ë™ì•„ë¦¬/êµìœ¡ê³¼ì • ì •ì  í˜ì´ì§€ë“¤ì„ ê³„ì† ì¶”ê°€
        self.department_static_urls = [
            # ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€
            {
                "url": "https://edison.kumoh.ac.kr/edison/sub0101.do",
                "name": "ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€ ì†Œê°œ",
            },
            {
                "url": "https://edison.kumoh.ac.kr/edison/sub0102.do",
                "name": "ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€ êµìœ¡ëª©í‘œ",
            },
            {
                "url": "https://edison.kumoh.ac.kr/edison/sub0104.do",
                "name": "ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€ ë¹„ì „",
            },

            # ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€
            {
                "url": "https://archi.kumoh.ac.kr/archi/sub0102.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ì†Œê°œ",
            },
            {
                "url": "https://archi.kumoh.ac.kr/archi/sub0103.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•í•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://archi.kumoh.ac.kr/archi/sub0104.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•ê³µí•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://civil.kumoh.ac.kr/civil/sub0101.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í† ëª©ê³µí•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://env.kumoh.ac.kr/env/sub0101.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://env.kumoh.ac.kr/env/sub0202_01.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ ì§€êµ¬í™˜ê²½ì—°êµ¬íšŒ ì†Œê°œ",
            },
            {
                "url": "https://env.kumoh.ac.kr/env/sub0202_02.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ ì•„ë¦„ë“œë¦¬ ì†Œê°œ",
            },
            {
                "url": "https://env.kumoh.ac.kr/env/sub0202_03.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ ESC ì†Œê°œ",
            },
            {
                "url": "https://env.kumoh.ac.kr/env/sub0202_04.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ BOD ì†Œê°œ",
            },

            # ê¸°ê³„ê³µí•™ë¶€
            {
                "url": "https://mecheng.kumoh.ac.kr/mecheng/sub0101.do",
                "name": "ê¸°ê³„ê³µí•™ë¶€ ê¸°ê³„ê³µí•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://mx.kumoh.ac.kr/md/sub0101.do",
                "name": "ê¸°ê³„ê³µí•™ë¶€ ê¸°ê³„ì‹œìŠ¤í…œê³µí•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://mobility.kumoh.ac.kr/smartmobility/sub0101.do",
                "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ ì¸ì‚¬ë§",
            },
            {
                "url": "https://mobility.kumoh.ac.kr/smartmobility/sub0102.do",
                "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ êµìœ¡ ëª©í‘œ",
            },
            {
                "url": "https://mobility.kumoh.ac.kr/smartmobility/sub0301.do",
                "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ ê³µë™í•™ê³¼ êµìœ¡ ê³¼ì •",
            },
            {
                "url": "https://mobility.kumoh.ac.kr/smartmobility/sub0304.do",
                "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ ì´ìˆ˜ì²´ê³„ë„",
            },

            # ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€
            {
                "url": "https://ie.kumoh.ac.kr/ie/sub0102.do",
                "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ì‚°ì—…ê³µí•™ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://ie.kumoh.ac.kr/ie/sub0603.do",
                "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ì‚°ì—…ê³µí•™ì „ê³µ ë™ì•„ë¦¬/í•™ìƒíšŒ",
            },
            {
                "url": "https://www.kumoh.ac.kr/bigdata/sub0102.do",
                "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ìˆ˜ë¦¬ë¹…ë°ì´í„°ì „ê³µ ê°œìš” ë° ì—°í˜",
            },
            {
                "url": "https://www.kumoh.ac.kr/bigdata/sub0502.do",
                "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ìˆ˜ë¦¬ë¹…ë°ì´í„°ì „ê³µ ì „ê³µë™ì•„ë¦¬",
            },

            # ì¬ë£Œê³µí•™ë¶€
            {
                "url": "https://polymer.kumoh.ac.kr/polymer/sub0202.do",
                "name": "ì¬ë£Œê³µí•™ë¶€ ê³ ë¶„ìê³µí•™ì „ê³µ ì „ê³µì†Œê°œ",
            },
            {
                "url": "https://polymer.kumoh.ac.kr/polymer/sub0502.do",
                "name": "ì¬ë£Œê³µí•™ë¶€ ê³ ë¶„ìê³µí•™ì „ê³µ ë™ì•„ë¦¬",
            },
            {
                "url": "https://mse.kumoh.ac.kr/mse/sub0102.do",
                "name": "ì¬ë£Œê³µí•™ë¶€ ì‹ ì†Œì¬ê³µí•™ì „ê³µ ì „ê³µì†Œê°œ",
            },
            {
                "url": "https://mse.kumoh.ac.kr/mse/sub020102.do",
                "name": "ì¬ë£Œê³µí•™ë¶€ ì‹ ì†Œì¬ê³µí•™ì „ê³µ êµìœ¡ê³¼ì • í¸ì„±í‘œ",
            },
            {
                "url": "https://mse.kumoh.ac.kr/mse/sub0602.do",
                "name": "ì¬ë£Œê³µí•™ë¶€ ì‹ ì†Œì¬ê³µí•™ì „ê³µ ë™ì•„ë¦¬",
            },

            # ì „ìê³µí•™ë¶€
            {
                "url": "https://see.kumoh.ac.kr/see/sub0101.do",
                "name": "ì „ìê³µí•™ë¶€ ë°˜ë„ì²´ì‹œìŠ¤í…œì „ê³µ ì „ìì‹œìŠ¤í…œì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://see.kumoh.ac.kr/see/sub0501.do",
                "name": "ì „ìê³µí•™ë¶€ ë°˜ë„ì²´ì‹œìŠ¤í…œì „ê³µ ì „ìì‹œìŠ¤í…œì „ê³µ ë™ì•„ë¦¬",
            },

            # ì»´í“¨í„°ê³µí•™ë¶€ - ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ
            {
                "url": "https://cs.kumoh.ac.kr/cs/sub0101.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì†Œê°œ",
            },
            {
                "url": "https://cs.kumoh.ac.kr/cs/sub0105_2.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ êµìœ¡ê³¼ì •",
            },
            {
                "url": "https://cs.kumoh.ac.kr/cs/sub0504.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ë™ì•„ë¦¬",
            },

            # ì»´í“¨í„°ê³µí•™ë¶€ - ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ
            {
                "url": "https://ai.kumoh.ac.kr/ai/sub0102.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ ê°œìš” ë° ì—°í˜",
            },
            {
                "url": "https://ai.kumoh.ac.kr/ai/sub0302.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •í‘œ",
            },
            {
                "url": "https://ai.kumoh.ac.kr/ai/sub0602.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ ì „ê³µë™ì•„ë¦¬",
            },

            # ì»´í“¨í„°ê³µí•™ë¶€ - ì»´í“¨í„°ê³µí•™ì „ê³µ
            {
                "url": "https://ce.kumoh.ac.kr/ce/sub0102.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì»´í“¨í„°ê³µí•™ì „ê³µ ê°œìš” ë° ì—°í˜",
            },
            {
                "url": "https://ce.kumoh.ac.kr/ce/sub0205.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì»´í“¨í„°ê³µí•™ì „ê³µ ë™ì•„ë¦¬",
            },
            {
                "url": "https://ce.kumoh.ac.kr/ce/sub0301.do",
                "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì»´í“¨í„°ê³µí•™ì „ê³µ êµê³¼ê³¼ì •",
            },

            # í™”í•™ì†Œì¬ê³µí•™ë¶€ - ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ
            {
                "url": "https://textile.kumoh.ac.kr/textile/sub0101.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ ì „ê³µì¥ ì¸ì‚¬ë§",
            },
            {
                "url": "https://textile.kumoh.ac.kr/textile/sub0203.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •",
            },
            {
                "url": "https://textile.kumoh.ac.kr/textile/sub0501.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ ì „ê³µë™ì•„ë¦¬",
            },

            # í™”í•™ì†Œì¬ê³µí•™ë¶€ - í™”í•™ê³µí•™ì „ê³µ
            {
                "url": "https://che.kumoh.ac.kr/che/sub0102.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ê³µí•™ì „ê³µ í•™ê³¼ì†Œê°œ",
            },
            {
                "url": "https://che.kumoh.ac.kr/che/sub0502.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ê³µí•™ì „ê³µ ë™ì•„ë¦¬",
            },

            # í™”í•™ì†Œì¬ê³µí•™ë¶€ - í™”í•™ìƒëª…ì†Œì¬ì „ê³µ
            {
                "url": "https://chembio.kumoh.ac.kr/chembio/sub0102.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ìƒëª…ì†Œì¬ì „ê³µ ì „ê³µê°œìš”",
            },

            # ê´‘ì‹œìŠ¤í…œê³µí•™ê³¼
            {
                "url": "https://optics.kumoh.ac.kr/optics/sub0101.do",
                "name": "ê´‘ì‹œìŠ¤í…œê³µí•™ê³¼ í•™ê³¼ì†Œê°œ",
            },

            # ë°”ì´ì˜¤ë©”ë””ì»¬ê³µí•™ê³¼
            {
                "url": "https://medicalit.kumoh.ac.kr/medicalit/sub0101.do",
                "name": "ë°”ì´ì˜¤ë©”ë””ì»¬ê³µí•™ê³¼ í•™ê³¼ì†Œê°œ",
            },
            {
                "url": "https://medicalit.kumoh.ac.kr/medicalit/sub020102.do",
                "name": "ë°”ì´ì˜¤ë©”ë””ì»¬ê³µí•™ê³¼ êµê³¼ì†Œê°œ",
            },

            # ITìœµí•©í•™ê³¼
            {
                "url": "https://itc.kumoh.ac.kr/itc/sub0101.do",
                "name": "ITìœµí•©í•™ê³¼ í•™ê³¼ì†Œê°œ",
            },
            {
                "url": "https://itc.kumoh.ac.kr/itc/sub0103.do#accordion-menu-title",
                "name": "ITìœµí•©í•™ê³¼ êµê³¼ëª©ê°œìš”",
            },

            # ììœ¨ì „ê³µí•™ë¶€
            {
                "url": "https://sls.kumoh.ac.kr/sls/sub0101.do",
                "name": "ììœ¨ì „ê³µí•™ë¶€ ì†Œê°œ",
            },
            {
                "url": "https://sls.kumoh.ac.kr/sls/sub0301.do",
                "name": "ììœ¨ì „ê³µí•™ë¶€ êµê³¼ê³¼ì •",
            },
            {
                "url": "https://sls.kumoh.ac.kr/sls/sub0302.do",
                "name": "ììœ¨ì „ê³µí•™ë¶€ ì „ê³µì„ íƒ",
            },

            # ê²½ì˜í•™ê³¼
            {
                "url": "https://biz.kumoh.ac.kr/biz/sub0102.do",
                "name": "ê²½ì˜í•™ê³¼ ì†Œê°œ",
            },
            {
                "url": "https://biz.kumoh.ac.kr/biz/sub0702.do",
                "name": "ê²½ì˜í•™ê³¼ ë™ì•„ë¦¬",
            },
        ]

        self.department_board_urls = [
            {
                "url": "https://archi.kumoh.ac.kr/archi/sub0201.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•í•™ì „ê³µ êµìœ¡ê³¼ì •"
            },
            {
                "url": "https://archi.kumoh.ac.kr/archi/sub0202.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"
            },
            {
                "url": "https://civil.kumoh.ac.kr/civil/sub030101.do",
                "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í† ëª©ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"
            },
            {
                "url": "https://ie.kumoh.ac.kr/ie/sub030101.do",
                "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ì‚°ì—…ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"
            },
            {
                "url": "https://www.kumoh.ac.kr/bigdata/sub030102.do",
                "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ìˆ˜ë¦¬ë¹…ë°ì´í„°ì „ê³µ êµìœ¡ê³¼ì •í‘œ"
            },
            {
                "url": "https://polymer.kumoh.ac.kr/polymer/sub0404.do",
                "name": "ì¬ë£Œê³µí•™ë¶€ ê³ ë¶„ìê³µí•™ì „ê³µ êµê³¼ê³¼ì •"
            },
            {
                "url": "https://che.kumoh.ac.kr/che/sub0304.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ê³µí•™ì „ê³µ êµê³¼ê³¼ì •"
            },
            {
                "url": "https://chembio.kumoh.ac.kr/chembio/sub030101.do",
                "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ìƒëª…ì†Œì¬ì „ê³µ êµìœ¡ê³¼ì • ë° êµê³¼ëª© ê°œìš”",
            },
            {
                "url": "https://optics.kumoh.ac.kr/optics/sub020102.do",
                "name": "ê´‘ì‹œìŠ¤í…œê³µí•™ê³¼ í•™ë¶€êµìœ¡ê³¼ì •"
            },
            {
                "url": "https://biz.kumoh.ac.kr/biz/sub030101.do",
                "name": "ê²½ì˜í•™ê³¼ êµê³¼ê³¼ì •"
            },
        ]

        # í•„í„° ë° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.quality_filter = QualityFilter(
            min_text_length=50,      # ì†Œê°œ í˜ì´ì§€ëŠ” ë„ˆë¬´ ë¹¡ì„¸ì§€ ì•Šê²Œ ì¡°ê¸ˆë§Œ ì™„í™”
            max_text_length=500000,
            min_word_count=10
        )

        output_dir = Path(__file__).parent.parent / "data" / "first_crawled"
        self.storage = JSONStorage(output_dir, pretty_print=True)

        self.content_extractor = ContentExtractor(
            keep_links=True,
            keep_images=False
        )

        # í†µê³„
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "filtered": 0,
            "skipped": 0,
            "attachments_found": 0,
            "attachments_uploaded": 0,
        }

        self.saved_pages = []
        self.existing_urls = set()
        self._load_existing_index()

    def _load_existing_index(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ì„ ì½ì–´ì„œ ì´ë¯¸ í¬ë¡¤ë§í•œ URL ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        output_dir = Path(__file__).parent.parent / "data" / "first_crawled"
        index_file = output_dir / "crawl_index.json"

        if index_file.exists():
            try:
                import json
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for page in data.get('pages', []):
                        url = page.get('url')
                        if url:
                            self.existing_urls.add(url)
                            self.saved_pages.append(page)
                logger.info(f"ğŸ“‚ ê¸°ì¡´ first í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ: {len(self.existing_urls)}ê°œ URL")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def crawl_url(self, url: str, page_info: dict) -> bool:
        """
        ë‹¨ì¼ ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ (í•™ê³¼ì†Œê°œ, ë™ì•„ë¦¬ì†Œê°œ, êµìœ¡ê³¼ì • ë“±)
        - ë‚ ì§œ í•„í„° ì—†ìŒ
        """
        self.stats["total"] += 1

        if url in self.existing_urls:
            logger.info(f"â­ï¸  ì´ë¯¸ í¬ë¡¤ë§ëœ URL - ê±´ë„ˆëœ€: {url}")
            self.stats["skipped"] += 1
            return False

        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            headers = {
                'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            html = response.text

            # í’ˆì§ˆ ê²€ì‚¬
            is_quality, reason = self.quality_filter.is_high_quality(html, url)
            if not is_quality:
                logger.warning(f"í’ˆì§ˆ í•„í„° ì‹¤íŒ¨: {reason}")
                self.stats["filtered"] += 1
                return False

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_data = self.content_extractor.extract_with_metadata(html)

            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ë° ì²˜ë¦¬
            attachments = self._process_attachments(url, html)

            # âœ… 1) page_typeì„ ì œì¼ ë¨¼ì € ê²°ì •
            page_type = page_info.get("page_type", "static_intro")

            # âœ… 2) board_name / title / display_title ì •ë¦¬
            if page_type == "static_intro":
                # ì •ì  ì†Œê°œ/ë¹„ì „/êµìœ¡ê³¼ì • í˜ì´ì§€:
                # - board_name: ì‚¬ì´íŠ¸ ì „ì²´ ê²½ë¡œ(HTML <title>)
                # - title/display_title: ì‚¬ëŒì´ ë³´ê¸° ì¢‹ì€ ì´ë¦„(page_info["name"])
                board_name = content_data["title"] or page_info["name"]
                title = page_info["name"]
                display_title = title
            else:
                # ê²Œì‹œíŒ ìµœì‹ ê¸€ ê°™ì€ ì¼€ì´ìŠ¤(board_notice ë“±)
                # - board_name: ìƒìœ„ ê²Œì‹œíŒ ì´ë¦„
                # - title: ê²Œì‹œê¸€ ì œëª©(ì¼ë‹¨ HTML <title> ì‚¬ìš©)
                board_name = page_info.get("board_name") or page_info["name"]
                title = content_data["title"] or page_info["name"]
                display_title = title          

            # ê²Œì‹œíŒ ìƒì„¸ í˜ì´ì§€ë©´ ì‘ì„±ì/ì¡°íšŒìˆ˜/ì‘ì„±ì¼ ì¶”ì¶œ
            author = None
            view_count = None
            created_at = None

            if "board_notice" in page_type or "latest" in page_info["name"]:
                try:
                    soup = BeautifulSoup(html, "html.parser")

                    # ì‘ì„±ì
                    el_author = soup.find(text="ì‘ì„±ì")
                    if el_author and el_author.parent:
                        author = el_author.parent.find_next().get_text(strip=True)

                    # ì¡°íšŒìˆ˜
                    el_view = soup.find(text="ì¡°íšŒ")
                    if el_view and el_view.parent:
                        view_count = el_view.parent.find_next().get_text(strip=True)
                        view_count = int(view_count) if view_count.isdigit() else None

                    # ì‘ì„±ì¼
                    el_date = soup.find(text="ì‘ì„±ì¼")
                    if el_date and el_date.parent:
                        created_raw = el_date.parent.find_next().get_text(strip=True)
                        # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        created_at = created_raw.replace('.', '-').strip()
                        try:
                            created_at = datetime.strptime(created_at, "%Y-%m-%d").isoformat()
                        except:
                            created_at = None

                except Exception as e:
                    print("[WARN] ê²Œì‹œíŒ ë©”íƒ€ íŒŒì‹± ì‹¤íŒ¨:", e)

            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "text_length": len(content_data['text']),
                "word_count": content_data['word_count'],
                "title": title,
                "board_name": board_name,
                "display_title": display_title,
                "paragraphs": content_data['paragraphs'],
                "link_count": len(content_data['links']),
                "attachments_count": len(attachments),
                "attachments": attachments,
                "images": content_data['images'],
                "quality_check": reason,
                "crawled_at": datetime.now().isoformat(),
                "source_url": url,
                "page_type": page_type,   # í•™ê³¼/ë™ì•„ë¦¬/ì†Œê°œ í˜ì´ì§€ íƒœê·¸
                "name": page_info["name"],
                "author": author,
                "view_count": view_count,
                "created_at": created_at,
            }

            # ì €ì¥
            filepath = self.storage.save_page(url, html, metadata)

            self.saved_pages.append({
                "url": url,
                "file": filepath,
                "title": content_data['title'],
                "text_length": len(content_data['text']),
                "page_type": metadata["page_type"],
            })

            self.existing_urls.add(url)
            self.stats["success"] += 1

            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {Path(filepath).name}")
            logger.info(f"   ì œëª©: {content_data['title'][:80]}...")
            logger.info(f"   ë³¸ë¬¸ ê¸¸ì´: {len(content_data['text'])} ë¬¸ì")
            logger.info(f"   ë¬¸ë‹¨ ìˆ˜: {content_data['paragraphs']}")

            # âœ… ì•ˆë“œë¡œì´ë“œ ì„œë²„ë¡œ ë©”íƒ€ë°ì´í„° ì „ì†¡ (í‚¤ì›Œë“œ í•„í„° ì ìš©)
            try:
                process_page(
                    url=url,
                    title=metadata["title"],
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ì•ˆë“œë¡œì´ë“œ ë©”íƒ€ë°ì´í„° ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return True

        except requests.RequestException as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False

        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False

    def _process_attachments(self, page_url: str, html: str) -> list:
        """
        HTMLì—ì„œ ì²¨ë¶€íŒŒì¼ ë§í¬ë¥¼ ì¶”ì¶œí•˜ê³  MinIOì— ì—…ë¡œë“œ

        - mode=download, .pdf, .hwp, .docx, .xlsx, .pptx, .zip ë“±
        - í•™ê³¼/ë™ì•„ë¦¬ ì†Œê°œ í˜ì´ì§€ì—ë„ êµìœ¡ê³¼ì • pdf ë“±ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì‚¬ìš©
        """
        attachments = []

        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = link.get_text(strip=True)

                is_download = (
                    'mode=download' in href or
                    'download' in href.lower() or
                    any(href.lower().endswith(ext) for ext in [
                        '.pdf', '.hwp', '.docx', '.xlsx', '.pptx', '.zip'
                    ])
                )

                if any(pattern in href for pattern in exclude_patterns):
                    is_download = False

                if not is_download:
                    continue

                # ì ˆëŒ€ URLë¡œ ë³€í™˜ (ë„ë©”ì¸ ìƒê´€ì—†ì´ ì•ˆì „í•˜ê²Œ)
                abs_url = urllib.parse.urljoin(page_url, href)

                self.stats["attachments_found"] += 1

                attachment_info = {
                    "page_url": page_url,
                    "link_text": link_text,
                    "download_url": abs_url,
                    "detected_at": datetime.now().isoformat(),
                }

                if self.enable_minio and self.minio:
                    try:
                        headers = {
                            'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)',
                            'Referer': page_url,
                        }
                        resp = requests.get(abs_url, headers=headers, timeout=30)
                        resp.raise_for_status()

                        file_data = resp.content
                        content_type = resp.headers.get('Content-Type', 'application/octet-stream')

                        # íŒŒì¼ëª… ì¶”ì¶œ
                        content_disp = resp.headers.get('Content-Disposition', '')
                        if 'filename=' in content_disp:
                            filename = content_disp.split('filename=')[-1].strip('"\'')
                        else:
                            filename = abs_url.split('/')[-1].split('?')[0]
                            if not filename or '.' not in filename:
                                if link_text and '.' in link_text:
                                    filename = link_text
                                else:
                                    filename = f"attachment_{hashlib.md5(abs_url.encode()).hexdigest()[:8]}.bin"

                        # URL ë””ì½”ë”©
                        try:
                            filename = urllib.parse.unquote(filename)
                        except Exception:
                            pass

                        # ê²½ë¡œ êµ¬ë¶„ì ì œê±°
                        clean_filename = filename.replace('/', '_').replace('\\', '_')
                        file_hash = hashlib.sha256(file_data).hexdigest()[:16]

                        object_name = f"attachments/{clean_filename}"
                        if self.minio.file_exists(object_name):
                            if '.' in clean_filename:
                                name_part, ext = clean_filename.rsplit('.', 1)
                                object_name = f"attachments/{name_part}_{file_hash[:8]}.{ext}"
                            else:
                                object_name = f"attachments/{clean_filename}_{file_hash[:8]}"

                        success, result = self.minio.upload_file(
                            file_data=file_data,
                            object_name=object_name,
                            content_type=content_type,
                            original_filename=filename,
                            metadata={
                                "source_url": abs_url,
                                "page_url": page_url,
                                "link_text": link_text,
                            }
                        )

                        if success:
                            attachment_info["minio_url"] = result
                            attachment_info["minio_object"] = object_name
                            attachment_info["file_size"] = len(file_data)
                            attachment_info["sha256"] = file_hash
                            attachment_info["filename"] = clean_filename
                            attachment_info["status"] = "uploaded"
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ: {clean_filename} ({len(file_data):,} bytes)")
                        else:
                            attachment_info["status"] = "upload_failed"
                            attachment_info["error"] = result
                            logger.warning(f"   âš ï¸  ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {filename}")
                    except Exception as e:
                        attachment_info["status"] = "download_failed"
                        attachment_info["error"] = str(e)
                        logger.warning(f"   âš ï¸  ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {link_text} - {e}")
                else:
                    attachment_info["status"] = "metadata_only"

                attachments.append(attachment_info)

            # 2) ì´ë¯¸ì§€(img src) ì²¨ë¶€ ì²˜ë¦¬
            image_exts = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg']

            for img in soup.find_all('img', src=True):
                src = img['src']
                alt_text = img.get('alt', '').strip()

                # 0) ì‚¬ì´íŠ¸ ê³µí†µ ì•„ì´ì½˜/ë¡œê³ /ë²„íŠ¼/ë°°ê²½ ì´ë¯¸ì§€ëŠ” ìŠ¤í‚µ
                if any(key in src for key in ICON_IMAGE_KEYWORDS):
                    continue

                # 1) í™•ì¥ì/íŒ¨í„´ ì²´í¬
                src_no_query = src.split('?', 1)[0].lower()
                is_image_by_ext = any(src_no_query.endswith(ext) for ext in image_exts)
                is_editor_image = 'editorimage.do' in src_no_query  # ë³¸ë¬¸ ì´ë¯¸ì§€

                if not (is_image_by_ext or is_editor_image):
                    continue

                # ì ˆëŒ€ URL ë³€í™˜
                abs_url = urllib.parse.urljoin(page_url, src)

                self.stats["attachments_found"] += 1

                attachment_info = {
                    "page_url": page_url,
                    "link_text": alt_text or "(image)",
                    "download_url": abs_url,
                    "detected_at": datetime.now().isoformat(),
                    "type": "image",   # â† ì´ë¯¸ì§€ íƒ€ì… í‘œì‹œ
                }

                if self.enable_minio and self.minio:
                    try:
                        headers = {
                            'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)'
                        }
                        resp = requests.get(abs_url, headers=headers, timeout=30)
                        resp.raise_for_status()

                        file_data = resp.content
                        content_type = resp.headers.get('Content-Type', 'image/*')

                        # íŒŒì¼ëª… ì¶”ì¶œ (URL ê¸°ì¤€)
                        filename = abs_url.split('/')[-1].split('?')[0]
                        if not filename:
                            filename = f"image_{hashlib.md5(abs_url.encode()).hexdigest()[:8]}.bin"

                        # URL ë””ì½”ë”©
                        try:
                            filename = urllib.parse.unquote(filename)
                        except Exception:
                            pass

                        clean_filename = filename.replace('/', '_').replace('\\', '_')
                        file_hash = hashlib.sha256(file_data).hexdigest()[:16]

                        object_name = f"images/{clean_filename}"
                        # ì´ë¯¸ ê°™ì€ object_nameì´ ìˆìœ¼ë©´ í•´ì‹œ ì¼ë¶€ë¥¼ ë¶™ì—¬ì„œ ì¶©ëŒ ë°©ì§€
                        if self.minio.file_exists(object_name):
                            if '.' in clean_filename:
                                name_part, ext = clean_filename.rsplit('.', 1)
                                object_name = f"images/{name_part}_{file_hash[:8]}.{ext}"
                            else:
                                object_name = f"images/{clean_filename}_{file_hash[:8]}"

                        success, result = self.minio.upload_file(
                            file_data=file_data,
                            object_name=object_name,
                            content_type=content_type,
                            original_filename=filename,
                            metadata={
                                "source_url": abs_url,
                                "page_url": page_url,
                                "alt_text": alt_text,
                            }
                        )

                        if success:
                            attachment_info["minio_url"] = result
                            attachment_info["minio_object"] = object_name
                            attachment_info["file_size"] = len(file_data)
                            attachment_info["sha256"] = file_hash
                            attachment_info["filename"] = clean_filename
                            attachment_info["status"] = "uploaded"
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ: {clean_filename} ({len(file_data):,} bytes)")
                        else:
                            attachment_info["status"] = "upload_failed"
                            attachment_info["error"] = result
                            logger.warning(f"   âš ï¸  ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {filename}")
                    except Exception as e:
                        attachment_info["status"] = "download_failed"
                        attachment_info["error"] = str(e)
                        logger.warning(f"   âš ï¸  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {alt_text or src} - {e}")
                else:
                    attachment_info["status"] = "metadata_only"

                attachments.append(attachment_info)

        except Exception as e:
            logger.error(f"âŒ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {e}")

        return attachments

    def crawl_latest_from_department_board(self, board_info):
        """
        êµìœ¡ê³¼ì • ê²Œì‹œíŒì—ì„œ 'ìµœì‹  ê²Œì‹œê¸€ 1ê°œë§Œ' í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
        """
        url = board_info["url"]
        name = board_info["name"]

        logger.info(f"\nğŸ“˜ [êµìœ¡ê³¼ì •] {name}: {url}")

        try:
            headers = {
                'User-Agent': 'KITBot/2.0 (CSEcapstone)'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° (ê¸°ì¡´ crawl_list_pageì™€ ë™ì¼í•œ ë°©ì‹)
            article_links = []

            for link in soup.find_all('a', href=True):
                href = link['href']
                if ('mode=view' in href) or ('articleNo' in href):
                    # ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                    if href.startswith('/'):
                        site_root = url.split('/', 3)[:3]  # https://archi.kumoh.ac.kr
                        base = "/".join(site_root)
                        full = base + href
                    elif href.startswith('?'):
                        full = url.split('?')[0] + href
                    else:
                        full = url.rsplit('/', 1)[0] + '/' + href

                    article_links.append(full)

            if not article_links:
                logger.warning(f"âŒ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í•¨: {url}")
                return False

            latest_url = article_links[0]
            logger.info(f"   ğŸ“Œ ìµœì‹  ê²Œì‹œê¸€: {latest_url}")

            # ì´ë¯¸ í¬ë¡¤ë§í•œ ê²½ìš° ìŠ¤í‚µ
            if latest_url in self.existing_urls:
                logger.info(f"   â­ï¸ ìµœì‹  ê²Œì‹œê¸€ ì´ë¯¸ í¬ë¡¤ë§ë¨ â†’ ìŠ¤í‚µ")
                self.stats["skipped"] += 1
                return False

            # ìµœì‹  ê²Œì‹œê¸€ í¬ë¡¤ë§
            page_info = {
                "url": latest_url,
                "name": f"{name} (ìµœì‹  ê²Œì‹œê¸€)",
                "page_type": "board_notice",
                "board_name": name,                
            }
            success = self.crawl_url(latest_url, page_info)

            if success:
                self.existing_urls.add(latest_url)

            return success

        except Exception as e:
            logger.error(f"âŒ êµìœ¡ê³¼ì • ê²Œì‹œíŒ ìµœì‹ ê¸€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False


    def run(self):
        """ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("=" * 80)
        print("departmentCrawler ì‹œì‘ (í•™ê³¼/ë™ì•„ë¦¬/ì •ì  ì†Œê°œ í˜ì´ì§€)")
        print("=" * 80)
        print(f"ëŒ€ìƒ URL ìˆ˜: {len(self.department_static_urls)}")
        print("=" * 80)

        start_time = datetime.now()

        # 1) ì •ì  í˜ì´ì§€(í•™ê³¼/ë™ì•„ë¦¬ ì†Œê°œ ë“±)
        for page in self.department_static_urls:
            print(f"\nğŸ“ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì´ë¦„ : [{page['name']}]")
            print("-" * 80)
            self.crawl_url(page['url'], page)
            import time
            time.sleep(0.5)

        # 2) í•™ê³¼ë³„ êµìœ¡ê³¼ì • ê²Œì‹œíŒ(ìµœì‹ ê¸€ 1ê°œì”©)
        print("\n" + "=" * 80)
        print("ğŸ“˜ í•™ê³¼ë³„ êµìœ¡ê³¼ì • ê²Œì‹œíŒ ìµœì‹ ê¸€ í¬ë¡¤ë§")
        print("=" * 80)

        for board in self.department_board_urls:
            print(f"\nğŸ“ ëŒ€ìƒ ê²Œì‹œíŒ ì´ë¦„ : [{board['name']}]")
            print("-" * 80)
            self.crawl_latest_from_department_board(board)
            import time
            time.sleep(0.5)

        # ì¸ë±ìŠ¤ ì €ì¥
        if self.saved_pages:
            index_data = {
                "crawl_date": datetime.now().isoformat(),
                "total_pages": len(self.saved_pages),
                "pages": self.saved_pages,
            }
            self.storage.save_index(index_data)
            logger.info(f"\nğŸ“š first ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(self.saved_pages)} í˜ì´ì§€")

        elapsed = datetime.now() - start_time

        print("\n" + "=" * 80)
        print("departmentCrawler í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 80)
        print(f"ì´ ì‹œë„: {self.stats['total']}")
        print(f"ì„±ê³µ: {self.stats['success']}")
        print(f"ê±´ë„ˆëœ€ (ì´ë¯¸ í¬ë¡¤ë§ë¨): {self.stats['skipped']}")
        print(f"ì‹¤íŒ¨: {self.stats['failed']}")
        print(f"í•„í„°ë¨: {self.stats['filtered']}")
        print(f"\nğŸ“ ì²¨ë¶€íŒŒì¼:")
        print(f"  - ë°œê²¬ë¨: {self.stats['attachments_found']}ê°œ")
        if self.enable_minio:
            print(f"  - MinIO ì—…ë¡œë“œ ì„±ê³µ: {self.stats['attachments_uploaded']}ê°œ")
        else:
            print(f"  - ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡ (MinIO ë¹„í™œì„±í™”)")
        print(f"\nì†Œìš” ì‹œê°„: {elapsed}")
        print("=" * 80)

        output_dir = Path(__file__).parent.parent / "data" / "first_crawled"
        print(f"\nğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        print(f"   - í˜ì´ì§€: {output_dir}/pages/")
        print(f"   - ì¸ë±ìŠ¤: {output_dir}/crawl_index.json")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='departmentCrawler - í•™ê³¼/ë™ì•„ë¦¬/ì†Œê°œ í˜ì´ì§€ 1íšŒì„± í¬ë¡¤ëŸ¬')
    parser.add_argument('--enable-minio', action='store_true',
                        help='ì²¨ë¶€íŒŒì¼ì„ MinIOì— ì—…ë¡œë“œ (ê¸°ë³¸ê°’: ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡)')
    args = parser.parse_args()

    crawler = departmentCrawler(enable_minio=args.enable_minio)
    crawler.run()


if __name__ == "__main__":
    main()
