#!/usr/bin/env python3
"""
ko_sitemap_static_crawler.py

https://www.kumoh.ac.kr/ko/ko.xml ì‚¬ì´íŠ¸ë§µì„ ëŒë©´ì„œ
/ko í•˜ìœ„ì˜ 'ì •ì  í˜ì´ì§€' ìœ„ì£¼ë¡œ departmentCrawlerë¥¼ ì´ìš©í•´ í¬ë¡¤ë§í•˜ëŠ” ì „ìš© ìŠ¤í¬ë¦½íŠ¸.
"""

import sys
import time
import requests
import xml.etree.ElementTree as ET
from pathlib import Path
from datetime import datetime

# departmentCrawler ëª¨ë“ˆ ì„í¬íŠ¸ ê°€ëŠ¥í•˜ë„ë¡ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from departmentCrawler import departmentCrawler, logger

EXCLUDED_URLS = {
    "http://www.kumoh.ac.kr/ko/sub01_02_03.do",      # ì—…ë¬´ì¶”ì§„ë¹„ ì‚¬ìš©ë‚´ì—­
    "http://www.kumoh.ac.kr/ko/sub01_05_01.do",      # KIT Projects
    "http://www.kumoh.ac.kr/ko/sub01_05_04.do",      # ë³´ë„ìë£Œ
    "http://www.kumoh.ac.kr/ko/sub06_01_01_01.do",   # ê³µì§€ì‚¬í•­ í•™ì‚¬ì•ˆë‚´
    "http://www.kumoh.ac.kr/ko/sub06_01_01_02.do",   # ê³µì§€ì‚¬í•­ í–‰ì‚¬ì•ˆë‚´
    "http://www.kumoh.ac.kr/ko/sub06_01_01_03.do",   # ê³µì§€ì‚¬í•­ ì¼ë°˜ì†Œì‹
    "http://www.kumoh.ac.kr/ko/sub06_03_04_02.do",   # ì •ë³´ê³µìœ  ê¸ˆì˜¤ë³µë•ë°©
    "http://www.kumoh.ac.kr/ko/sub06_03_04_04.do",   # ì •ë³´ê³µìœ  ì•„ë¥´ë°”ì´íŠ¸ì •ë³´
    "http://www.kumoh.ac.kr/ko/sub06_03_05_01.do",   # ë¬¸í™”ì˜ˆìˆ ê³µê°„ í´ë˜ì‹ê°ìƒ
    "http://www.kumoh.ac.kr/ko/sub06_03_05_02.do",   # ë¬¸í™”ì˜ˆìˆ ê³µê°„ ê°¤ëŸ¬ë¦¬
    "http://www.kumoh.ac.kr/ko/sub06_05_02.do",      # ì´ì¥ì„ìš©í›„ë³´ìì¶”ì²œìœ„ì›íšŒ ê³µì§€ì‚¬í•­
    "http://www.kumoh.ac.kr/ko/sub01_01_07_02.do",  # ëŒ€í•™ì†Œê°œ í˜„í™© ì¬ì •í˜„í™©
    "http://www.kumoh.ac.kr/ko/sub01_01_07_03.do",  # ëŒ€í•™ì†Œê°œ í˜„í™© ì¬ì •ìœ„ì›íšŒ íšŒì˜ë¡
    "http://www.kumoh.ac.kr/ko/sub01_01_07_04.do",  # ëŒ€í•™ì†Œê°œ í˜„í™© ëŒ€í•™í‰ì˜ì›íšŒ íšŒì˜ë¡
    "http://www.kumoh.ac.kr/ko/sub01_01_07_05.do",  # ëŒ€í•™ì†Œê°œ í˜„í™© ë“±ë¡ê¸ˆì‹¬ì˜ìœ„ì›íšŒ íšŒì˜ë¡
    "http://www.kumoh.ac.kr/ko/sub01_01_08.do",     # ëŒ€í•™ì†Œê°œ UI
    "http://www.kumoh.ac.kr/ko/sub01_04.do",        # ëŒ€í•™ì†Œê°œ ê·œì •ì§‘
    "http://www.kumoh.ac.kr/ko/sub01_05_02.do",     # KIT People
    "http://www.kumoh.ac.kr/ko/sub01_05_03.do",     # KIT News
    "http://www.kumoh.ac.kr/ko/sub07_01_02.do",     # ê¸ˆì˜¤ì‹ ë¬¸ê³  ì²­íƒê¸ˆì§€ë²•ìë£Œì‹¤
    "http://www.kumoh.ac.kr/ko/sub07_01_03.do",     # ê¸ˆì˜¤ì‹ ë¬¸ê³  í–‰ë™ê°•ë ¹ìë£Œì‹¤
}

def crawl_static_from_sitemap(
    crawler: departmentCrawler,
    sitemap_url: str = "http://www.kumoh.ac.kr/ko/ko.xml",
):
    """
    /ko ì‚¬ì´íŠ¸ë§µì„ ëŒë©´ì„œ ì •ì  í˜ì´ì§€ í›„ë³´ë§Œ ê³¨ë¼ departmentCrawler.crawl_url()ì— ë„˜ê¸´ë‹¤.
    """
    logger.info(f"\nğŸ—º  ì‚¬ì´íŠ¸ë§µ í¬ë¡¤ë§ ì‹œì‘: {sitemap_url}")

    try:
        headers = {
            "User-Agent": "KITBot/2.0 (CSEcapstone, sitemap-crawler)"
        }
        resp = requests.get(sitemap_url, headers=headers, timeout=15)
        resp.raise_for_status()

        root = ET.fromstring(resp.content)

        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì—¬ë¶€ ìƒê´€ì—†ì´ <loc> íƒœê·¸ ì „ë¶€ ì°¾ê¸°
        loc_elems = root.findall(".//{*}loc")
        raw_urls = [e.text.strip() for e in loc_elems if e.text]

        logger.info(f"   ì‚¬ì´íŠ¸ë§µ URL ê°œìˆ˜: {len(raw_urls)}")

        static_urls = []
        seen = set()

        for u in raw_urls:
            # 0) ìŠ¤í‚´ ì •ê·œí™” (http â†’ https)
            if u.startswith("http://"):
                u = "https://" + u[len("http://"):]
            # í˜¹ì‹œ ë‹¤ë¥¸ ë„ë©”ì¸ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ /ko/ë§Œ í•„í„°
            if not u.startswith("https://www.kumoh.ac.kr/ko/"):
                continue

            # ğŸ”¹ URL ì •ê·œí™” (ë’¤ì— ìŠ¬ë˜ì‹œ ì •ë¦¬)
            normalized = u.rstrip("/")

            # ğŸ”¹ 1) ëª…ì‹œ ì œì™¸ URLì´ë©´ ë°”ë¡œ ìŠ¤í‚µ
            if normalized in EXCLUDED_URLS:
                logger.info(f"   â­ï¸ ì œì™¸ URL ìŠ¤í‚µ: {normalized}")
                continue

            # ë™ì /ê²Œì‹œíŒ/ê²€ìƒ‰/ë¡œê·¸ì¸/íŒŒì¼ë‹¤ìš´ë¡œë“œ ë“± ì œì™¸
            if any(pat in u for pat in [
                "mode=", "articleNo=", "search", "Search",
                "login", "Login",
                "fileDownload", "fileDown", "download=",
                "board", "bbs", "reg.do",
            ]):
                continue

            # ì§ì ‘ íŒŒì¼ ë§í¬(ppt, pdf ë“±) ì œì™¸
            if any(u.lower().endswith(ext) for ext in [
                ".pdf", ".hwp", ".xls", ".xlsx",
                ".ppt", ".pptx", ".zip",
            ]):
                continue

            # ì•µì»¤(#) í¬í•¨ í˜ì´ì§€ëŠ” ì¤‘ë³µ/ë¶ˆí•„ìš”í•œ ê²½ìš°ê°€ ë§ìœ¼ë‹ˆ ì œì™¸
            if "#" in u:
                continue

            # 5) ì¤‘ë³µ ì œê±°
            if normalized in seen:
                continue
            seen.add(normalized)
            static_urls.append(normalized)

        logger.info(f"   í•„í„° í›„ ì •ì  í›„ë³´ URL ìˆ˜: {len(static_urls)}")

        for url in static_urls:
            page_info = {
                "url": url,
                "name": url,           # ë³„ë„ ì´ë¦„ì´ ì—†ìœ¼ë‹ˆ URL ìì²´ë¥¼ nameìœ¼ë¡œ ì‚¬ìš©
                "page_type": "static_intro",
            }
            print(f"\nğŸ“ ì‚¬ì´íŠ¸ë§µ ì •ì  í˜ì´ì§€ : [{url}]")
            print("-" * 80)
            crawler.crawl_url(url, page_info)
            time.sleep(0.5)

    except Exception as e:
        logger.error(f"âŒ ì‚¬ì´íŠ¸ë§µ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="KO ì‚¬ì´íŠ¸ë§µ ê¸°ë°˜ ì •ì  í˜ì´ì§€ í¬ë¡¤ëŸ¬ (departmentCrawler ì¬ì‚¬ìš©)"
    )
    parser.add_argument(
        "--enable-minio",
        action="store_true",
        help="ì²¨ë¶€íŒŒì¼ì„ MinIOì— ì—…ë¡œë“œ (ê¸°ë³¸ê°’: ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡)",
    )
    parser.add_argument(
        "--sitemap-url",
        default="https://www.kumoh.ac.kr/ko/ko.xml",
        help="ëŒ€ìƒ ì‚¬ì´íŠ¸ë§µ URL (ê¸°ë³¸: /ko/ko.xml)",
    )

    args = parser.parse_args()

    # departmentCrawler ì¬ì‚¬ìš© (ì €ì¥ ìœ„ì¹˜, ì¸ë±ìŠ¤ êµ¬ì¡°, ì²¨ë¶€ ì²˜ë¦¬ ëª¨ë‘ ê·¸ëŒ€ë¡œ í™œìš©)
    crawler = departmentCrawler(enable_minio=args.enable_minio)

    print("=" * 80)
    print("KO ì‚¬ì´íŠ¸ë§µ ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 80)
    print(f"ì‚¬ì´íŠ¸ë§µ: {args.sitemap_url}")
    print("=" * 80)

    start_time = datetime.now()
    crawl_static_from_sitemap(crawler, args.sitemap_url)
    elapsed = datetime.now() - start_time

    # ì¸ë±ìŠ¤ ì €ì¥ (departmentCrawlerì™€ ë™ì¼ í¬ë§·)
    if crawler.saved_pages:
        index_data = {
            "crawl_date": datetime.now().isoformat(),
            "total_pages": len(crawler.saved_pages),
            "pages": crawler.saved_pages,
        }
        crawler.storage.save_index(index_data)
        logger.info(f"\nğŸ“š first ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(crawler.saved_pages)} í˜ì´ì§€")

    print("\n" + "=" * 80)
    print("KO ì‚¬ì´íŠ¸ë§µ ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ì´ ì‹œë„: {crawler.stats['total']}")
    print(f"ì„±ê³µ: {crawler.stats['success']}")
    print(f"ê±´ë„ˆëœ€ (ì´ë¯¸ í¬ë¡¤ë§ë¨): {crawler.stats['skipped']}")
    print(f"ì‹¤íŒ¨: {crawler.stats['failed']}")
    print(f"í•„í„°ë¨: {crawler.stats['filtered']}")
    print(f"\nğŸ“ ì²¨ë¶€íŒŒì¼:")
    print(f"  - ë°œê²¬ë¨: {crawler.stats['attachments_found']}ê°œ")
    if crawler.enable_minio:
        print(f"  - MinIO ì—…ë¡œë“œ ì„±ê³µ: {crawler.stats['attachments_uploaded']}ê°œ")
    else:
        print(f"  - ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡ (MinIO ë¹„í™œì„±í™”)")
    print(f"\nì†Œìš” ì‹œê°„: {elapsed}")
    print("=" * 80)

    output_dir = Path(__file__).parent.parent / "data" / "first_crawled"
    print(f"\nğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"   - í˜ì´ì§€: {output_dir}/pages/")
    print(f"   - ì¸ë±ìŠ¤: {output_dir}/crawl_index.json")


if __name__ == "__main__":
    main()
