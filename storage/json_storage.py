"""
JSON í¬ë§·ìœ¼ë¡œ í¬ë¡¤ë§ ë°ì´í„° ì €ì¥
â†’ ëª¨ë“  JSONì„ 'ì •ê·œí™”ëœ ë¬¸ì„œ' í˜•íƒœë¡œ ì €ì¥
"""
import json
from pathlib import Path
from datetime import datetime
from typing import Optional
import hashlib
from urllib.parse import urlparse, parse_qs


def _guess_site_from_url(url: str) -> str:
    """
    URLì—ì„œ site ì½”ë“œ ì¶”ì¶œ
    ì˜ˆ) https://bus.kumoh.ac.kr/...  â†’ 'bus'
        https://mobility.kumoh.ac.kr/... â†’ 'mobility'
    """
    parsed = urlparse(url)
    host = (parsed.hostname or "")
    parts = host.split(".")
    if len(parts) >= 3 and parts[-2:] == ["ac", "kr"]:
        return parts[0]
    return host or ""


def _slug_from_path(path: str) -> str:
    """
    /smartmobility/sub0301.do â†’ smartmobility_sub0301
    /bus/notice.do?mode=view â†’ bus_notice
    """
    p = path.strip("/")
    if not p:
        return "root"
    slug = p.replace("/", "_")
    slug = slug.replace(".do", "").replace(".jsp", "")
    return slug


class JSONStorage:
    def __init__(self, output_dir: Path, pretty_print: bool = False):
        """
        Args:
            output_dir: JSON íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
            pretty_print: JSONì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…í• ì§€ ì—¬ë¶€
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.pretty_print = pretty_print
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ë‹´ì„ íŒŒì¼
        self.index_file = self.output_dir / "crawl_index.json"
        self.pages_dir = self.output_dir / "pages"
        self.pages_dir.mkdir(exist_ok=True)

    # ---------------- ì •ê·œí™” ë¬¸ì„œ ë¹Œë” ---------------- #

    def _build_normalized_doc(
        self,
        url: str,
        title: str,
        text: str,
        metadata: dict,
        crawled_at: str,
    ) -> dict:
        """
        ëª¨ë“  í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë‘ ë²ˆì§¸ ì˜ˆì‹œ í˜•íƒœì˜ 'ì •ê·œí™” ë¬¸ì„œ'ë¡œ ë³€í™˜
        """
        parsed = urlparse(url)
        site = metadata.get("site") or _guess_site_from_url(url)
        page_type = metadata.get("page_type", "page")  # ê¸°ë³¸ê°’: ì¼ë°˜ í˜ì´ì§€

        # ê³µí†µ í•„ë“œ ì´ˆê¸°ê°’
        doc = {
            "doc_id": None,
            "source_type": None,           # "board" or "page" ë“±
            "site": site,
            "board_name": metadata.get("board_name"),
            "title": metadata.get("title"),
            "display_title": metadata.get("display_title") or metadata.get("title"),
            "author": metadata.get("author"),
            "url": url,
            "created_at": metadata.get("created_at"),
            "updated_at": metadata.get("updated_at"),
            "has_explicit_date": bool(metadata.get("created_at")),
            "view_count": metadata.get("view_count"),
            "doc_type": "html",
            "main_text": text,
            "attachments": metadata.get("attachments", []),
            "images": metadata.get("images", []),
            "crawled_at": crawled_at,
        }

        # 1) ê²Œì‹œíŒ íƒ€ì… (ê³µì§€/ë‰´ìŠ¤ ë“±) â†’ bus_notice_514537 ê°™ì€ doc_id
        if page_type == "board_notice":
            qs = parse_qs(parsed.query)
            article_no = qs.get("articleNo", [""])[0]
            if article_no:
                doc["doc_id"] = f"{site}_notice_{article_no}"
            else:
                slug = _slug_from_path(parsed.path)
                doc["doc_id"] = f"{site}_notice_{slug}"
            doc["source_type"] = "board"

        # 2) ê·¸ ì™¸(ì •ì  í˜ì´ì§€ / ì¼ë°˜ HTML í˜ì´ì§€)
        else:
            slug = _slug_from_path(parsed.path)
            doc["doc_id"] = f"{site}_page_{slug}"
            doc["source_type"] = "page"

            # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ê³  ì´ë¯¸ì§€ë§Œ ìˆëŠ” í˜ì´ì§€ëŠ” íƒ€ì… êµ¬ë¶„
            if not text and metadata.get("images"):
                doc["doc_type"] = "image_html"

        return doc

    # ---------------- ì‹¤ì œ ì €ì¥ í•¨ìˆ˜ ---------------- #

    def save_page(
        self,
        url: str,
        html: str,
        metadata: dict = None,
        extracted_text: str = None,
        title: str = None,
    ) -> str:
        """
        í˜ì´ì§€ë¥¼ JSON(ì •ê·œí™” ë¬¸ì„œ)ìœ¼ë¡œ ì €ì¥
        
        Args:
            url: í˜ì´ì§€ URL
            html: HTML ì½˜í…ì¸ 
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            extracted_text: ì´ë¯¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ìˆìœ¼ë©´ ì¬ì¶”ì¶œ ì•ˆí•¨)
            title: ì´ë¯¸ ì¶”ì¶œëœ ì œëª©
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        # URL ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±
        url_hash = hashlib.sha256(url.encode()).hexdigest()[:16]
        filename = f"{url_hash}.json"
        filepath = self.pages_dir / filename

        # í…ìŠ¤íŠ¸/ì œëª© ì—†ìœ¼ë©´ ì¶”ì¶œ
        if extracted_text is None:
            from filters.content_extractor import ContentExtractor
            extractor = ContentExtractor(keep_links=True, keep_images=False)
            content_data = extractor.extract_with_metadata(html)
            text = content_data["text"]
            title_text = content_data["title"]
        else:
            text = extracted_text
            title_text = title if title else "ì œëª© ì—†ìŒ"

        crawled_at = datetime.now().isoformat()

        # ë©”íƒ€ë°ì´í„° ì •ë¦¬
        meta = metadata.copy() if metadata else {}
        meta.setdefault("title", title_text)
        meta.setdefault("text_length", len(text))
        meta.setdefault("word_count", len(text.split()) if text else 0)
        meta.setdefault("crawled_at", crawled_at)
        meta.setdefault("source_url", url)

        # ğŸ”¹ ì—¬ê¸°ì„œ ìµœì¢… ì •ê·œí™” ë¬¸ì„œ ìƒì„± (ë‘ ë²ˆì§¸ JSON í˜•íƒœ)
        doc = self._build_normalized_doc(
            url=url,
            title=title_text,
            text=text,
            metadata=meta,
            crawled_at=crawled_at,
        )

        # JSON ì €ì¥
        with open(filepath, "w", encoding="utf-8") as f:
            if self.pretty_print:
                json.dump(doc, f, ensure_ascii=False, indent=2)
            else:
                json.dump(doc, f, ensure_ascii=False)
        
        return str(filepath)
    
    def save_index(self, index_data):
        """
        í¬ë¡¤ë§ëœ ëª¨ë“  í˜ì´ì§€ì˜ ì¸ë±ìŠ¤ ì €ì¥
        
        Args:
            index_data: dict ë˜ëŠ” list
                - dictë©´ ê·¸ëŒ€ë¡œ ì €ì¥ (meta ì •ë³´ í¬í•¨ ê°€ëŠ¥)
                - listë©´ pagesë¡œ ê°ì‹¸ì„œ ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)
        """
        # í•˜ìœ„ í˜¸í™˜ì„±: listê°€ ì˜¤ë©´ dictë¡œ ë³€í™˜
        if isinstance(index_data, list):
            index_data = {
                "crawl_date": datetime.now().isoformat(),
                "total_pages": len(index_data),
                "pages": index_data
            }
        
        with open(self.index_file, 'w', encoding='utf-8') as f:
            if self.pretty_print:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            else:
                json.dump(index_data, f, ensure_ascii=False)
    
    def load_page(self, filepath: str) -> Optional[dict]:
        """JSON íŒŒì¼ì—ì„œ ì •ê·œí™”ëœ ë¬¸ì„œ ë¡œë“œ"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None
    
    def load_index(self) -> Optional[dict]:
        """ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ"""
        if not self.index_file.exists():
            return None
        
        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None
