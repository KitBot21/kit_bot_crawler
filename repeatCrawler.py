#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬ - 2ê°œ ì„¹ì…˜ë§Œ
1. ì¼ì •: /ko/schedule_reg.do
2. ê³µì§€ì‚¬í•­(í•™ì‚¬ì•ˆë‚´): /ko/sub06_01_01_01.do
"""
import sys
import requests
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env")

# crawler ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from filters.content_extractor import ContentExtractor
from filters.quality_filter import QualityFilter
from filters.date_filter import DateFilter
from storage.json_storage import JSONStorage
from storage.minio_storage import MinIOStorage
import logging
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

exclude_patterns = [
    "/cms/fileDownload.do",
]

class SimpleTestCrawler:
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, enable_minio: bool = False):
        """
        Args:
            enable_minio: MinIO ì‚¬ìš© ì—¬ë¶€ (Trueë©´ ì²¨ë¶€íŒŒì¼ì„ MinIOì— ì—…ë¡œë“œ)
        """
        self.base_url = "https://www.kumoh.ac.kr"
        self.bus_base_url = "https://bus.kumoh.ac.kr"
        
        # MinIO ì„¤ì •
        self.enable_minio = enable_minio
        if enable_minio:
            try:
                self.minio = MinIOStorage.from_env()
                logger.info("âœ… MinIO ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  MinIO ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                logger.warning("   ì²¨ë¶€íŒŒì¼ì€ ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡ë©ë‹ˆë‹¤.")
                self.enable_minio = False
                self.minio = None
        else:
            self.minio = None
        
        # í¬ë¡¤ë§í•  URL ëª©ë¡
        self.target_urls = [
            # í•™ì‚¬ì¼ì •
            "https://www.kumoh.ac.kr/ko/schedule_reg.do",
            # êµë‚´ ì‹ë‹¹
            "https://www.kumoh.ac.kr/ko/restaurant01.do",
            "https://www.kumoh.ac.kr/ko/restaurant02.do",
            "https://www.kumoh.ac.kr/ko/restaurant04.do",
            "https://www.kumoh.ac.kr/ko/restaurant05.do",
            # ìƒí™œê´€ ì‹ë‹¹
            "https://www.kumoh.ac.kr/dorm/restaurant_menu01.do",
            "https://www.kumoh.ac.kr/dorm/restaurant_menu02.do",
            "https://www.kumoh.ac.kr/dorm/restaurant_menu03.do",
        ]
        
        # ê²Œì‹œíŒ URL (ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ - ì—¬ëŸ¬ ê²Œì‹œê¸€ í¬ë¡¤ë§)
        self.board_urls = [
            {
                "url": "https://bus.kumoh.ac.kr/bus/notice.do",
                "name": "í†µí•™ë²„ìŠ¤ ê³µì§€",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                # ëŒ€ë¶€ë¶„ì˜ ê²Œì‹œê¸€ì´ ì‚¬ì§„
                "url": "https://www.kumoh.ac.kr/ko/sub01_02_03.do",
                "name": "ì—…ë¬´ì¶”ì§„ë¹„ ì‚¬ìš©ë‚´ì—­",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_05_01.do",
                "name": "KIT Projects",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                # ì²¨ë¶€íŒŒì¼ ì¡´ì¬í•˜ì§€ë§Œ, ë³¸ë¬¸ê³¼ ë‚´ìš©ì´ ë™ì¼
                "url": "https://www.kumoh.ac.kr/ko/sub01_05_04.do",
                "name": "ë³´ë„ìë£Œ",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                # ë‹¤ì–‘í•˜ê²Œ ì¡´ì¬
                "url": "https://www.kumoh.ac.kr/ko/sub06_01_01_01.do",
                "name": "ê³µì§€ì‚¬í•­ í•™ì‚¬ì•ˆë‚´",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                # ë‹¤ì–‘í•˜ê²Œ ì¡´ì¬
                "url": "https://www.kumoh.ac.kr/ko/sub06_01_01_02.do",
                "name": "ê³µì§€ì‚¬í•­ í–‰ì‚¬ì•ˆë‚´",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub06_01_01_03.do",
                "name": "ê³µì§€ì‚¬í•­ ì¼ë°˜ì†Œì‹",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub06_03_04_02.do",
                "name": "ì •ë³´ê³µìœ  ê¸ˆì˜¤ë³µë•ë°©",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub06_03_04_04.do",
                "name": "ì •ë³´ê³µìœ  ì•„ë¥´ë°”ì´íŠ¸ì •ë³´",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub06_03_05_01.do",
                "name": "ë¬¸í™”ì˜ˆìˆ ê³µê°„ í´ë˜ì‹ê°ìƒ",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                # ì‚¬ì§„ë§Œ ì¡´ì¬
                "url": "https://www.kumoh.ac.kr/ko/sub06_03_05_02.do",
                "name": "ë¬¸í™”ì˜ˆìˆ ê³µê°„ ê°¤ëŸ¬ë¦¬",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                # zipíŒŒì¼ ì¡´ì¬
                "url": "https://www.kumoh.ac.kr/ko/sub06_05_02.do",
                "name": "ì´ì¥ì„ìš©í›„ë³´ìì¶”ì²œìœ„ì›íšŒ ê³µì§€ì‚¬í•­",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/dorm/sub0401.do",
                "name": "ìƒí™œê´€ ê³µì§€ì‚¬í•­",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/dorm/sub0407.do",
                "name": "ìƒí™œê´€ ì„ ë°œ ê³µì§€ì‚¬í•­",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/dorm/sub0408.do",
                "name": "ìƒí™œê´€ ì…í‡´ì‚¬ ê³µì§€ì‚¬í•­",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
            {
                "url": "https://www.kumoh.ac.kr/dorm/sub0603.do",
                "name": "ì‹ í‰ë™ ì‹ ì²­ë°©ë²•",
                "max_pages": 0,  # 0 = ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
                "skip_date_filter": False,  # ë‚ ì§œ í•„í„° ì ìš© (2021-01-01 ì´í›„ë§Œ)
            },
                        {
                "url": "https://www.kumoh.ac.kr/ko/sub01_01_07_02.do",
                "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ì¬ì •í˜„í™©",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_01_07_03.do",
                "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ì¬ì •ìœ„ì›íšŒ íšŒì˜ë¡",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_01_07_04.do",
                "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ëŒ€í•™í‰ì˜ì›íšŒ íšŒì˜ë¡",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_01_07_05.do",
                "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ë“±ë¡ê¸ˆì‹¬ì˜ìœ„ì› íšŒì˜ë¡",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_01_08.do",
                "name": "ëŒ€í•™ì†Œê°œ UI",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_02_03.do",
                "name": "ëŒ€í•™ì†Œê°œ ì—´ë¦°ì´ì¥ì‹¤ ì—…ë¬´ì¶”ì§„ë¹„",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_04.do",
                "name": "ëŒ€í•™ì†Œê°œ ê·œì •ì§‘",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_05_02.do",
                "name": "ëŒ€í•™ì†Œê°œ í™ë³´ KIT People",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub01_05_03.do",
                "name": "ëŒ€í•™ì†Œê°œ í™ë³´ KIT News",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub07_01_02.do",
                "name": "ê¸ˆì˜¤ì‹ ë¬¸ê³  ì²­íƒê¸ˆì§€ë²•ìë£Œì‹¤",
                "max_pages": 0,
                "skip_date_filter": False,
            },
            {
                "url": "https://www.kumoh.ac.kr/ko/sub07_01_03.do",
                "name": "ê¸ˆì˜¤ì‹ ë¬¸ê³  í–‰ë™ê°•ë ¹ìë£Œì‹¤",
                "max_pages": 0,
                "skip_date_filter": False,
            },
        ]
        
        # í•„í„° ë° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.quality_filter = QualityFilter(
            min_text_length=100,
            max_text_length=500000,
            min_word_count=20
        )
        
        # ë‚ ì§œ í•„í„° (2021-01-01 ì´í›„ë§Œ)
        self.date_filter = DateFilter(cutoff_date="2021-01-01")
        
        output_dir = Path(__file__).parent.parent / "data" / "test_crawled"
        self.storage = JSONStorage(output_dir, pretty_print=True)
        
        self.content_extractor = ContentExtractor(keep_links=True, keep_images=False)
        
        # í†µê³„
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "filtered": 0,
            "filtered_date": 0,  # ë‚ ì§œ í•„í„°ë¡œ ì œì™¸ëœ ìˆ˜
            "skipped": 0,  # ì´ë¯¸ í¬ë¡¤ë§ëœ í˜ì´ì§€
            "attachments_found": 0,  # ë°œê²¬ëœ ì²¨ë¶€íŒŒì¼
            "attachments_uploaded": 0,  # MinIO ì—…ë¡œë“œ ì„±ê³µ
        }
        
        self.saved_pages = []
        
        # ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ
        self.existing_urls = set()
        self.index_meta = {}  # ë©”íƒ€ ì •ë³´ ì´ˆê¸°í™”
        self._load_existing_index()
    
    def _load_existing_index(self):
        """ê¸°ì¡´ í¬ë¡¤ë§ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ì²´í¬"""
        index_file = Path(__file__).parent.parent / "data" / "test_crawled" / "crawl_index.json"
        if index_file.exists():
            try:
                import json
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for page in data.get('pages', []):
                        self.existing_urls.add(page['url'])
                        # ê¸°ì¡´ í˜ì´ì§€ë¥¼ saved_pagesì—ë„ ì¶”ê°€
                        self.saved_pages.append(page)
                    
                    # ë©”íƒ€ ì •ë³´ ì €ì¥ (ì²« í•­ëª©, í¬ë¡¤ë§ ë‚ ì§œ ë“±)
                    self.index_meta = data.get('meta', {})
                    
                logger.info(f"âœ… ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ: {len(self.existing_urls)}ê°œ URL")
            except Exception as e:
                logger.warning(f"âš ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)
        self.existing_urls = set()
        self._load_existing_index()
    
    def _load_existing_index(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ì„ ì½ì–´ì„œ ì´ë¯¸ í¬ë¡¤ë§í•œ URL ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
        output_dir = Path(__file__).parent.parent / "data" / "test_crawled"
        index_file = output_dir / "crawl_index.json"
        
        if index_file.exists():
            try:
                import json
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for page in data.get('pages', []):
                        self.existing_urls.add(page['url'])
                        # ê¸°ì¡´ í˜ì´ì§€ë„ saved_pagesì— ì¶”ê°€
                        self.saved_pages.append(page)
                logger.info(f"ğŸ“‚ ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ: {len(self.existing_urls)}ê°œ URL")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def crawl_url(self, url: str, skip_date_filter: bool = False, context: dict | None = None) -> bool:
        """
        ë‹¨ì¼ URL í¬ë¡¤ë§

        Args:
            url: í¬ë¡¤ë§í•  URL
            skip_date_filter: Trueë©´ ë‚ ì§œ í•„í„° ê±´ë„ˆë›°ê¸° (í•™ì‚¬ì¼ì • ë“±)
            context: ê²Œì‹œíŒ ì´ë¦„, ì†ŒìŠ¤ íƒ€ì… ë“± ë¶€ê°€ ì •ë³´
                    ì˜ˆ) {"source_type": "board", "board_name": "í†µí•™ë²„ìŠ¤ ê³µì§€"}
        """
        self.stats["total"] += 1
        context = context or {}

        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            headers = {
                'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            html = response.text

            # ê²Œì‹œê¸€ ì‘ì„±ì¼(ì •í™•í•œ 'ì‘ì„±ì¼') ë¨¼ì € ì‹œë„
            post_date = self._extract_post_date(html)

            # ë‚ ì§œ í•„í„° (í•™ì‚¬ì¼ì • ë“±ì€ ìŠ¤í‚µ)
            if not skip_date_filter:
                # ë‚ ì§œ ì¶”ì¶œ (ê²Œì‹œê¸€ì˜ ì‘ì„±ì¼ì´ ìˆìœ¼ë©´ ê·¸ê±¸, ì—†ìœ¼ë©´ ë°±ì—… ì¶”ì¶œ)
                date_str = post_date or self._extract_date_from_html(html)

                # ë‚ ì§œ í•„í„° ì²´í¬ (2021-01-01 ì´í›„ë§Œ)
                if date_str and not self.date_filter.is_recent(date_str):
                    logger.info(f"  â­ï¸  ë‚ ì§œ í•„í„°: {date_str} (2021-01-01 ì´ì „)")
                    self.stats["filtered"] += 1
                    self.stats["filtered_date"] += 1
                    return False

            # ê¸°ë³¸ ê°’ë“¤
            author = None
            view_count = None
            created_at = post_date  # ê¸°ë³¸ì€ ì‘ì„±ì¼(YYYY-MM-DD)
            has_explicit_date = bool(created_at)

            # ê²Œì‹œíŒ ê¸€ì´ë©´ author / view / created_at í•œ ë²ˆ ë” ì •í™•íˆ íŒŒì‹±
            if context.get("source_type") == "board":
                b_author, b_view, b_created = self._extract_board_meta(html)
                if b_author:
                    author = b_author
                if b_view is not None:
                    view_count = b_view
                if b_created:
                    created_at = b_created
                    has_explicit_date = True

            # í’ˆì§ˆ ê²€ì‚¬
            is_quality, reason = self.quality_filter.is_high_quality(html, url)
            if not is_quality:
                logger.warning(f"í’ˆì§ˆ í•„í„° ì‹¤íŒ¨: {reason}")
                self.stats["filtered"] += 1
                return False

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_data = self.content_extractor.extract_with_metadata(html)

            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ë° ì²˜ë¦¬
            attachments = self._process_attachments(url, html)

            # ê²Œì‹œíŒì´ë©´ ì œëª©ì„ ë”°ë¡œ í•œ ë²ˆ ë” ì‹œë„
            board_title = None
            if context.get("source_type") == "board":
                board_title = self._extract_board_title(html)

            title_for_json = board_title or content_data['title'] or context.get("board_name")

            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (JSONStorageê°€ ì´ê±¸ ë³´ê³  ìƒë‹¨ í•„ë“œ ìƒì„±)
            metadata = {
                "text_length": len(content_data['text']),
                "word_count": content_data['word_count'],
                "title": title_for_json,
                "paragraphs": content_data['paragraphs'],
                "link_count": len(content_data['links']),
                "attachments_count": len(attachments),
                "attachments": attachments,
                "images": content_data['images'],
                "quality_check": reason,
                "crawled_at": datetime.now().isoformat(),

                # ì¶”ê°€ëœ ë¶€ë¶„ë“¤ â†“
                "source_url": url,
                "source_type": context.get("source_type", "page"),  # "page" or "board"
                "board_name": content_data['title'],
                "author": author,
                "view_count": view_count,
                "created_at": created_at,
                "has_explicit_date": has_explicit_date,
            }

            # ì €ì¥ (ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì™€ ì œëª©ì„ ë„˜ê²¨ì„œ main_text / title ì„¸íŒ…)
            filepath = self.storage.save_page(
                url=url,
                html=html,
                metadata=metadata,
                extracted_text=content_data['text'],
                title=title_for_json,
            )

            self.saved_pages.append({
                "url": url,
                "file": filepath,
                "title": title_for_json,
                "text_length": len(content_data['text']),
            })

            self.stats["success"] += 1

            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {Path(filepath).name}")
            logger.info(f"   ì œëª©: {content_data['title'][:50]}...")
            logger.info(f"   ë³¸ë¬¸ ê¸¸ì´: {len(content_data['text'])} ë¬¸ì")
            logger.info(f"   ë¬¸ë‹¨ ìˆ˜: {content_data['paragraphs']}")

            return True

        except requests.RequestException as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False

        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False
        
    def _extract_board_title(self, html: str) -> str | None:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html, 'html.parser')
        head = soup.find('div', class_='title-area')
        if not head:
            return None

        # ê¸ˆì˜¤ ê²Œì‹œíŒì€ ë³´í†µ h4, strong ì•ˆì— ì œëª©ì´ ë“¤ì–´ìˆìŒ
        for tag in ['h4', 'h3', 'strong']:
            el = head.find(tag)
            if el:
                text = el.get_text(strip=True)
                if text:
                    return text

        return None    

    def _extract_board_meta(self, html: str):
        """
        ê¸ˆì˜¤ ê²Œì‹œíŒ ìƒì„¸ í˜ì´ì§€ì˜ ìƒë‹¨ ì •ë³´(ì‘ì„±ì, ì¡°íšŒìˆ˜, ì‘ì„±ì¼) íŒŒì‹±
        ë°˜í™˜: (author, view_count, created_at)  created_atì€ YYYY-MM-DD ë˜ëŠ” None
        """
        from bs4 import BeautifulSoup
        import re

        soup = BeautifulSoup(html, 'html.parser')
        info_div = soup.find('div', class_='board-view-information')
        author = None
        view_count = None
        created_at = None

        if not info_div:
            return author, view_count, created_at

        for dl in info_div.find_all('dl'):
            dt = dl.find('dt')
            dd = dl.find('dd')
            if not dt or not dd:
                continue

            key = dt.get_text(strip=True)
            val = dd.get_text(strip=True)

            if 'ì‘ì„±ì' in key:
                author = val
            elif 'ì¡°íšŒ' in key:
                digits = ''.join(ch for ch in val if ch.isdigit())
                if digits:
                    view_count = int(digits)
            elif 'ì‘ì„±ì¼' in key:
                m = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', val)
                if m:
                    y, mth, d = m.groups()
                    created_at = f"{y}-{mth}-{d}"

        return author, view_count, created_at

    def _process_attachments(self, page_url: str, html: str) -> list:
        """
        HTMLì—ì„œ ì²¨ë¶€íŒŒì¼ ë§í¬ë¥¼ ì¶”ì¶œí•˜ê³  MinIOì— ì—…ë¡œë“œ
        
        Args:
            page_url: í˜„ì¬ í˜ì´ì§€ URL
            html: HTML ì†ŒìŠ¤
            
        Returns:
            ì²¨ë¶€íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        attachments = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸° (mode=download, .hwp, .pdf ë“±)
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = link.get_text(strip=True)
                
                # ë‹¤ìš´ë¡œë“œ íŒ¨í„´ í™•ì¸
                is_download = (
                    'mode=download' in href or
                    'download' in href.lower() or
                    any(href.lower().endswith(ext) for ext in ['.pdf', '.hwp', '.docx', '.xlsx', '.pptx', '.zip'])
                )

                if any(pattern in href for pattern in exclude_patterns):
                    is_download = False
                
                if not is_download:
                    continue
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if href.startswith('?'):
                    abs_url = page_url.split('?')[0] + href
                elif href.startswith('/'):
                    # ë„ë©”ì¸ ê²°ì •
                    if 'bus.kumoh.ac.kr' in page_url:
                        abs_url = f"{self.bus_base_url}{href}"
                    else:
                        abs_url = f"{self.base_url}{href}"
                elif not href.startswith('http'):
                    abs_url = f"{page_url.rsplit('/', 1)[0]}/{href}"
                else:
                    abs_url = href
                
                self.stats["attachments_found"] += 1
                
                # ì²¨ë¶€íŒŒì¼ ì •ë³´ ê¸°ë¡
                attachment_info = {
                    "page_url": page_url,
                    "link_text": link_text,
                    "download_url": abs_url,
                    "detected_at": datetime.now().isoformat(),
                }
                
                # MinIO ì—…ë¡œë“œ ì‹œë„
                if self.enable_minio and self.minio:
                    try:
                        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                        headers = {
                            'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)',
                            'Referer': page_url
                        }
                        response = requests.get(abs_url, headers=headers, timeout=30)
                        response.raise_for_status()
                        
                        file_data = response.content
                        content_type = response.headers.get('Content-Type', 'application/octet-stream')
                        
                        # íŒŒì¼ëª… ì¶”ì¶œ
                        content_disp = response.headers.get('Content-Disposition', '')
                        if 'filename=' in content_disp:
                            filename = content_disp.split('filename=')[-1].strip('"\'')
                        else:
                            # URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                            filename = abs_url.split('/')[-1].split('?')[0]
                            if not filename or '.' not in filename:
                                # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ í™•ì¥ì ì¶”ì¶œ ì‹œë„
                                if link_text and '.' in link_text:
                                    filename = link_text
                                else:
                                    filename = f"attachment_{hashlib.md5(abs_url.encode()).hexdigest()[:8]}.bin"
                        
                        # MinIO ê°ì²´ ì´ë¦„ ìƒì„± (í•œê¸€ íŒŒì¼ëª… ì‚¬ìš©)
                        file_hash = hashlib.sha256(file_data).hexdigest()[:16]
                        
                        # URL ë””ì½”ë”© (ì¸ì½”ë”©ëœ íŒŒì¼ëª…ì´ ìˆìœ¼ë©´ ë³µì›)
                        import urllib.parse
                        try:
                            filename = urllib.parse.unquote(filename)
                        except:
                            pass
                        
                        # íŒŒì¼ëª… ì •ë¦¬ (ê²½ë¡œ êµ¬ë¶„ìë§Œ ì œê±°)
                        clean_filename = filename.replace('/', '_').replace('\\', '_')
                        
                        # ì¤‘ë³µ ë°©ì§€: ê°™ì€ ì´ë¦„ì´ ìˆìœ¼ë©´ í•´ì‹œ ì¶”ê°€
                        object_name = f"attachments/{clean_filename}"
                        
                        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì¤‘ë³µì´ë©´ í•´ì‹œ ì¶”ê°€
                        if self.minio.file_exists(object_name):
                            # í™•ì¥ì ë¶„ë¦¬
                            if '.' in clean_filename:
                                name_part, ext = clean_filename.rsplit('.', 1)
                                object_name = f"attachments/{name_part}_{file_hash[:8]}.{ext}"
                            else:
                                object_name = f"attachments/{clean_filename}_{file_hash[:8]}"
                        
                        # ì´ë¯¸ ì—…ë¡œë“œëœ íŒŒì¼ì¸ì§€ í™•ì¸ (í•´ì‹œë¡œ)
                        # MinIOì— ì—…ë¡œë“œ (original_filename ì¶”ê°€)
                        success, result = self.minio.upload_file(
                            file_data=file_data,
                            object_name=object_name,
                            content_type=content_type,
                            original_filename=filename,
                            metadata={
                                "source_url": abs_url,
                                "page_url": page_url,
                                "link_text": link_text
                            }
                        )
                        
                        if success:
                            attachment_info["minio_url"] = result
                            attachment_info["minio_object"] = object_name
                            attachment_info["file_size"] = len(file_data)
                            attachment_info["sha256"] = file_hash
                            attachment_info["filename"] = clean_filename
                            attachment_info["status"] = "uploaded"
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ: {clean_filename} ({len(file_data):,} bytes)")
                        else:
                            attachment_info["status"] = "upload_failed"
                            attachment_info["error"] = result
                            logger.warning(f"   âš ï¸  ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {filename}")
                    except Exception as e:
                        attachment_info["status"] = "download_failed"
                        attachment_info["error"] = str(e)
                        logger.warning(f"   âš ï¸  ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {link_text} - {e}")
                else:
                    attachment_info["status"] = "metadata_only"
                
                attachments.append(attachment_info)
        
            # 2) ì´ë¯¸ì§€(img src) ì²¨ë¶€ ì²˜ë¦¬
            image_exts = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg']

            for img in soup.find_all('img', src=True):
                src = img['src']
                alt_text = img.get('alt', '').strip()

                # í™•ì¥ì í•„í„° (ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œê±° í›„ íŒë³„)
                src_no_query = src.split('?', 1)[0].lower()
                if not any(src_no_query.endswith(ext) for ext in image_exts):
                    continue

                # í•„ìš”í•˜ë©´ exclude_patterns ì¬ì‚¬ìš© (ëŒ€ë¶€ë¶„ì€ ì•ˆ ê±¸ë¦¬ê² ì§€ë§Œ í†µì¼ê° ì°¨ì›ì—ì„œ)
                if any(pattern in src for pattern in exclude_patterns):
                    continue

                # ì ˆëŒ€ URL ë³€í™˜
                abs_url = urllib.parse.urljoin(page_url, src)

                self.stats["attachments_found"] += 1

                attachment_info = {
                    "page_url": page_url,
                    "link_text": alt_text or "(image)",
                    "download_url": abs_url,
                    "detected_at": datetime.now().isoformat(),
                    "type": "image",   # â† ì´ë¯¸ì§€ íƒ€ì… í‘œì‹œ
                }

                if self.enable_minio and self.minio:
                    try:
                        headers = {
                            'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)',
                            'Referer': page_url,
                        }
                        resp = requests.get(abs_url, headers=headers, timeout=30)
                        resp.raise_for_status()

                        file_data = resp.content
                        content_type = resp.headers.get('Content-Type', 'image/*')

                        # íŒŒì¼ëª… ì¶”ì¶œ (URL ê¸°ì¤€)
                        filename = abs_url.split('/')[-1].split('?')[0]
                        if not filename:
                            filename = f"image_{hashlib.md5(abs_url.encode()).hexdigest()[:8]}.bin"

                        # URL ë””ì½”ë”©
                        try:
                            filename = urllib.parse.unquote(filename)
                        except Exception:
                            pass

                        clean_filename = filename.replace('/', '_').replace('\\', '_')
                        file_hash = hashlib.sha256(file_data).hexdigest()[:16]

                        object_name = f"images/{clean_filename}"
                        # ì´ë¯¸ ê°™ì€ object_nameì´ ìˆìœ¼ë©´ í•´ì‹œ ì¼ë¶€ë¥¼ ë¶™ì—¬ì„œ ì¶©ëŒ ë°©ì§€
                        if self.minio.file_exists(object_name):
                            if '.' in clean_filename:
                                name_part, ext = clean_filename.rsplit('.', 1)
                                object_name = f"images/{name_part}_{file_hash[:8]}.{ext}"
                            else:
                                object_name = f"images/{clean_filename}_{file_hash[:8]}"

                        success, result = self.minio.upload_file(
                            file_data=file_data,
                            object_name=object_name,
                            content_type=content_type,
                            original_filename=filename,
                            metadata={
                                "source_url": abs_url,
                                "page_url": page_url,
                                "alt_text": alt_text,
                            }
                        )

                        if success:
                            attachment_info["minio_url"] = result
                            attachment_info["minio_object"] = object_name
                            attachment_info["file_size"] = len(file_data)
                            attachment_info["sha256"] = file_hash
                            attachment_info["filename"] = clean_filename
                            attachment_info["status"] = "uploaded"
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ: {clean_filename} ({len(file_data):,} bytes)")
                        else:
                            attachment_info["status"] = "upload_failed"
                            attachment_info["error"] = result
                            logger.warning(f"   âš ï¸  ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {filename}")
                    except Exception as e:
                        attachment_info["status"] = "download_failed"
                        attachment_info["error"] = str(e)
                        logger.warning(f"   âš ï¸  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {alt_text or src} - {e}")
                else:
                    attachment_info["status"] = "metadata_only"

                attachments.append(attachment_info)

        except Exception as e:
            logger.error(f"âŒ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {e}")
        
        return attachments
    
    def _extract_post_date(self, html: str):
        """
        ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ì‘ì„±ì¼'ë§Œ ì •í™•íˆ ì¶”ì¶œ
        ë°˜í™˜ í˜•ì‹: YYYY-MM-DD ë˜ëŠ” None
        """
        from bs4 import BeautifulSoup
        import re

        soup = BeautifulSoup(html, 'html.parser')

        # board-view-information ë¸”ëŸ­ ì°¾ê¸°
        info_div = soup.find('div', class_='board-view-information')
        if not info_div:
            return None

        # <dl><dt>ì‘ì„±ì¼</dt><dd>2025.11.20</dd> êµ¬ì¡° íƒìƒ‰
        for dl in info_div.find_all('dl'):
            dt = dl.find('dt')
            dd = dl.find('dd')
            if not dt or not dd:
                continue

            dt_text = dt.get_text(strip=True)
            if 'ì‘ì„±ì¼' not in dt_text:
                continue

            raw = dd.get_text(strip=True)  # ì˜ˆ: "2025.11.20"
            m = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', raw)
            if not m:
                return None

            year, month, day = m.groups()
            # YYYY-MM-DD í˜•íƒœë¡œ ë¦¬í„´
            return f"{year}-{month}-{day}"

        return None

    def _extract_date_from_html(self, html: str) -> str:
        """
        HTMLì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        ê²Œì‹œê¸€ì˜ ì‘ì„±ì¼, ìˆ˜ì •ì¼ ë“±ì„ ì°¾ìŠµë‹ˆë‹¤.
        
        Returns:
            ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD í˜•ì‹) ë˜ëŠ” None
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # íŒ¨í„´ 1: <dd> íƒœê·¸ì—ì„œ ë‚ ì§œ ì°¾ê¸° (ê¸ˆì˜¤ê³µëŒ€ ê²Œì‹œíŒ íŒ¨í„´)
        for dd in soup.find_all('dd'):
            text = dd.get_text(strip=True)
            # YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD í˜•ì‹
            import re
            date_match = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', text)
            if date_match:
                year, month, day = date_match.groups()
                return f"{year}-{month}-{day}"
        
        # íŒ¨í„´ 2: classë‚˜ idì— 'date' í¬í•¨ëœ ìš”ì†Œ
        for elem in soup.find_all(class_=re.compile('date|time', re.I)):
            text = elem.get_text(strip=True)
            date_match = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', text)
            if date_match:
                year, month, day = date_match.groups()
                return f"{year}-{month}-{day}"
        
        # íŒ¨í„´ 3: meta íƒœê·¸
        for meta in soup.find_all('meta'):
            if meta.get('property') in ['article:published_time', 'article:modified_time']:
                content = meta.get('content', '')
                date_match = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', content)
                if date_match:
                    year, month, day = date_match.groups()
                    return f"{year}-{month}-{day}"
        
        return None
    
    def crawl_list_page(self, url: str, max_pages: int = 10, skip_date_filter: bool = False, board_name: str = "ê²Œì‹œíŒ"):
        """
        ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ (ê²Œì‹œíŒ ëª©ë¡)
        
        Args:
            url: ê²Œì‹œíŒ ëª©ë¡ URL
            max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (0 = ëª¨ë“  í˜ì´ì§€)
            skip_date_filter: Trueë©´ ë‚ ì§œ í•„í„° ê±´ë„ˆë›°ê¸°
            board_name: ê²Œì‹œíŒ ì´ë¦„ (ë¡œê·¸ ì¶œë ¥ìš©)
        """
        logger.info(f"\nğŸ“‹ [{board_name}] ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¶„ì„: {url}")
        
        page_num = 0
        total_articles = 0
        
        # base_url ê²°ì • (í†µí•™ë²„ìŠ¤ëŠ” ë‹¤ë¥¸ ë„ë©”ì¸)
        if 'bus.kumoh.ac.kr' in url:
            base_url = self.bus_base_url
        else:
            base_url = self.base_url
        
        while True:
            # í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ URL ìƒì„±
            if page_num == 0:
                page_url = url
            else:
                # í˜ì´ì§€ë„¤ì´ì…˜ URL íŒ¨í„´ (ê¸ˆì˜¤ê³µëŒ€ëŠ” article.offset ì‚¬ìš©)
                offset = page_num * 10  # í•œ í˜ì´ì§€ì— 10ê°œì”©
                if '?' in url:
                    page_url = f"{url}&article.offset={offset}"
                else:
                    page_url = f"{url}?article.offset={offset}"
            
            try:
                headers = {
                    'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)'
                }
                response = requests.get(page_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
                article_links = []
                
                # íŒ¨í„´: mode=view í¬í•¨
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if 'mode=view' in href or 'articleNo' in href:
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if href.startswith('/'):
                            full_url = f"{base_url}{href}"
                        elif href.startswith('?'):
                            full_url = f"{url.split('?')[0]}{href}"
                        elif not href.startswith('http'):
                            # ë„ë©”ì¸ì— ë”°ë¼ ê¸°ë³¸ ê²½ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •
                            if 'bus.kumoh.ac.kr' in url:
                                full_url = f"{base_url}/bus/{href}"
                            else:
                                full_url = f"{base_url}/ko/{href}"
                        else:
                            full_url = href
                        
                        if full_url not in article_links:
                            article_links.append(full_url)
                
                if not article_links:
                    logger.info(f"   í˜ì´ì§€ {page_num + 1}: ê²Œì‹œê¸€ ì—†ìŒ - ì¢…ë£Œ")
                    break
                
                logger.info(f"   í˜ì´ì§€ {page_num + 1}: {len(article_links)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
                
                # ê° ê²Œì‹œê¸€ í¬ë¡¤ë§
                for i, article_url in enumerate(article_links, 1):
                    # ì´ë¯¸ í¬ë¡¤ë§í•œ URLì¸ì§€ í™•ì¸
                    if article_url in self.existing_urls:
                        logger.info(f"\n   [{total_articles + i}] ì´ë¯¸ í¬ë¡¤ë§ë¨ - ê±´ë„ˆëœ€: {article_url[:60]}...")
                        self.stats["skipped"] += 1
                        continue
                    
                    logger.info(f"\n   [{total_articles + i}] {article_url[:80]}...")

                    # âœ… ê²Œì‹œíŒ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
                    context = {
                        "source_type": "board",
                        "board_name": board_name,   # í•¨ìˆ˜ ì¸ìë¡œ ë°›ì€ board_name
                    }

                    success = self.crawl_url(
                        article_url,
                        skip_date_filter=skip_date_filter,
                        context=context,
                    )
                    
                    # í¬ë¡¤ë§ ì„±ê³µ ì‹œ existing_urlsì— ì¶”ê°€
                    if success:
                        self.existing_urls.add(article_url)
                    
                    # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    import time
                    time.sleep(0.7)
                
                total_articles += len(article_links)
                page_num += 1
                
                # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì²´í¬
                if max_pages > 0 and page_num >= max_pages:
                    logger.info(f"\n   ìµœëŒ€ í˜ì´ì§€ ìˆ˜({max_pages}) ë„ë‹¬ - ì¢…ë£Œ")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ
                import time
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ í˜ì´ì§€ {page_num + 1} ì—ëŸ¬: {e}")
                break
        
        logger.info(f"\nâœ… [{board_name}] ì´ {total_articles}ê°œ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì™„ë£Œ")
    
    def crawl_schedule_lists(self, url: str, max_pages: int = 0):
        """
        í•™ì‚¬ì¼ì • ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë“¤ì„ í¬ë¡¤ë§
        ê° í˜ì´ì§€ì˜ ì¼ì • ëª©ë¡ì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            url: í•™ì‚¬ì¼ì • ë©”ì¸ URL
            max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (0 = ëª¨ë“  í˜ì´ì§€)
        """
        logger.info(f"\nğŸ“‹ í•™ì‚¬ì¼ì • ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§: {url}")
        
        page_num = 0
        
        while True:
            # í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ URL ìƒì„±
            if page_num == 0:
                page_url = url
            else:
                # í˜ì´ì§€ë„¤ì´ì…˜ URL íŒ¨í„´
                offset = page_num * 10  # í•œ í˜ì´ì§€ì— 10ê°œì”©
                if '?' in url:
                    page_url = f"{url}&article.offset={offset}"
                else:
                    page_url = f"{url}?article.offset={offset}"
            
            try:
                headers = {
                    'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)'
                }
                
                # ì¤‘ë³µ ì²´í¬
                if page_url in self.existing_urls:
                    logger.info(f"   í˜ì´ì§€ {page_num + 1}: ì´ë¯¸ í¬ë¡¤ë§ë¨ - ê±´ë„ˆëœ€")
                    self.stats["skipped"] += 1
                    page_num += 1
                    continue
                
                response = requests.get(page_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                
                # í•™ì‚¬ì¼ì • í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
                # tbody ì•ˆì— trì´ ìˆëŠ”ì§€ ì²´í¬
                table_rows = soup.find_all('tr')
                schedule_rows = []
                
                for row in table_rows:
                    # í•™ì‚¬ì¼ì • ë°ì´í„° í–‰ì¸ì§€ í™•ì¸ (tdê°€ ìˆê³  ë‚ ì§œ í˜•ì‹ í¬í•¨)
                    tds = row.find_all('td')
                    if len(tds) >= 5:  # ë²ˆí˜¸, ì œëª©, ì‹œì‘ì¼, ì¢…ë£Œì¼, ë“±ë¡ì¼ ë“±
                        schedule_rows.append(row)
                
                # ì²« í˜ì´ì§€ì¸ ê²½ìš° ìµœì‹  í•­ëª© ì²´í¬ (íš¨ìœ¨ì  ì¤‘ë³µ ê°ì§€)
                if page_num == 0:
                    # ì²« í•­ëª© ì •ë³´ ì¶”ì¶œ
                    first_schedule = None
                    if schedule_rows:
                        first_row = schedule_rows[0]
                        cells = first_row.find_all('td')
                        if len(cells) >= 2:
                            # ë²ˆí˜¸, ì œëª©, ì‹œì‘ì¼ ë“±ì„ ì¡°í•©
                            first_schedule = "|".join([cell.get_text(strip=True) for cell in cells[:3]])
                    
                    # ì´ì „ ì²« í•­ëª©ê³¼ ë¹„êµ
                    prev_first = self.index_meta.get('schedule_first_item')
                    if prev_first and first_schedule and prev_first == first_schedule:
                        logger.info(f"   âœ… ìµœì‹  ì¼ì • ë³€ê²½ ì—†ìŒ - ì „ì²´ ìŠ¤í‚µ")
                        break
                    elif first_schedule:
                        logger.info(f"   ğŸ†• ìƒˆë¡œìš´ ì¼ì • ê°ì§€ - ì „ì²´ ì¬í¬ë¡¤ë§")
                        # ë©”íƒ€ ì •ë³´ ì—…ë°ì´íŠ¸
                        self.index_meta['schedule_first_item'] = first_schedule
                        self.index_meta['schedule_last_update'] = datetime.now().isoformat()
                
                if not schedule_rows:
                    logger.info(f"   í˜ì´ì§€ {page_num + 1}: ì¼ì • ì—†ìŒ - ì¢…ë£Œ")
                    break
                
                logger.info(f"   í˜ì´ì§€ {page_num + 1}: {len(schedule_rows)}ê°œ ì¼ì • ë°œê²¬")
                
                # ì´ í˜ì´ì§€ ì „ì²´ë¥¼ ì €ì¥
                self.stats["total"] += 1
                
                # í’ˆì§ˆ ê²€ì‚¬
                is_quality, reason = self.quality_filter.is_high_quality(html, page_url)
                if not is_quality:
                    logger.warning(f"   í’ˆì§ˆ í•„í„° ì‹¤íŒ¨: {reason}")
                    self.stats["filtered"] += 1
                    page_num += 1
                    continue
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_data = self.content_extractor.extract_with_metadata(html)
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    "text_length": len(content_data['text']),
                    "word_count": content_data['word_count'],
                    "title": f"{content_data['title']} - í˜ì´ì§€ {page_num + 1}",
                    "paragraphs": content_data['paragraphs'],
                    "page_number": page_num + 1,
                    "schedule_count": len(schedule_rows),
                    "type": "schedule_list",
                    "quality_check": reason,
                    "crawled_at": datetime.now().isoformat(),
                }
                
                # ì €ì¥
                filepath = self.storage.save_page(page_url, html, metadata)
                
                self.saved_pages.append({
                    "url": page_url,
                    "file": filepath,
                    "title": metadata['title'],
                    "text_length": len(content_data['text']),
                    "page_number": page_num + 1,
                    "schedule_count": len(schedule_rows),
                })
                
                self.stats["success"] += 1
                
                logger.info(f"   âœ… ì €ì¥ ì™„ë£Œ: {Path(filepath).name}")
                logger.info(f"      ì¼ì • ê°œìˆ˜: {len(schedule_rows)}")
                logger.info(f"      ë³¸ë¬¸ ê¸¸ì´: {len(content_data['text'])} ë¬¸ì")
                
                page_num += 1
                
                # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì²´í¬
                if max_pages > 0 and page_num >= max_pages:
                    logger.info(f"\n   ìµœëŒ€ í˜ì´ì§€ ìˆ˜({max_pages}) ë„ë‹¬ - ì¢…ë£Œ")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ
                import time
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ í˜ì´ì§€ {page_num + 1} ì—ëŸ¬: {e}")
                break
        
        logger.info(f"\nâœ… ì´ {page_num}ê°œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
    
    def crawl_restaurant_lists(self, url: str, max_pages: int = 1):
        """
        ì‹ë‹¹ ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë“¤ì„ í¬ë¡¤ë§
        ì²« í˜ì´ì§€ì—ë§Œ ë©”ë‰´ í…Œì´ë¸”ì´ ìˆìœ¼ë¯€ë¡œ ì²« í˜ì´ì§€ë§Œ í¬ë¡¤ë§
        
        Args:
            url: ì‹ë‹¹ ë©”ë‰´ í˜ì´ì§€ URL
            max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1, ì²« í˜ì´ì§€ë§Œ)
        """
        logger.info(f"\nğŸ½ï¸ ì‹ë‹¹ ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§: {url}")
        
        page_num = 0
        
        # ì²« í˜ì´ì§€ë§Œ í¬ë¡¤ë§ (ë©”ë‰´ í…Œì´ë¸”ì´ ì²« í˜ì´ì§€ì—ë§Œ ìˆìŒ)
        while page_num < max_pages:
            page_url = url
            
            # ì¤‘ë³µ ì²´í¬
            if page_url in self.existing_urls:
                # ì‹ë‹¹ ë©”ë‰´ëŠ” ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œë„ ì²´í¬ (ë§¤ì¼ ì—…ë°ì´íŠ¸)
                restaurant_key = url.split('/')[-1]  # restaurant01.do ë“±
                last_crawl = self.index_meta.get(f'{restaurant_key}_last_crawl')
                
                if last_crawl:
                    last_date = datetime.fromisoformat(last_crawl).date()
                    today = datetime.now().date()
                    
                    if last_date >= today:
                        logger.info(f"   ì´ë¯¸ ì˜¤ëŠ˜ í¬ë¡¤ë§ë¨ - ê±´ë„ˆëœ€")
                        self.stats["skipped"] += 1
                        break
                    else:
                        logger.info(f"   ğŸ†• ë‚ ì§œ ë³€ê²½ ê°ì§€ ({last_date} â†’ {today}) - ì¬í¬ë¡¤ë§")
                        # ê¸°ì¡´ URL ì œê±°í•˜ê³  ì¬í¬ë¡¤ë§
                        self.existing_urls.discard(page_url)
                else:
                    logger.info(f"   ì´ë¯¸ í¬ë¡¤ë§ë¨ - ê±´ë„ˆëœ€")
                    self.stats["skipped"] += 1
                    break
            
            try:
                headers = {
                    'User-Agent': 'KITBot/2.0 (CSEcapstone, contact: cdh5113@naver.com)'
                }
                response = requests.get(page_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                
                # ë©”ë‰´ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
                table_rows = soup.find_all('tr')
                menu_rows = []
                
                for row in table_rows:
                    # ë©”ë‰´ ë°ì´í„° í–‰ì¸ì§€ í™•ì¸
                    tds = row.find_all('td')
                    if len(tds) >= 3:  # ë‚ ì§œ, ë©”ë‰´ ì •ë³´ ë“±
                        menu_rows.append(row)
                
                # ë©”ë‰´ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ë‹¨, ì²« í˜ì´ì§€ëŠ” í•­ìƒ ì €ì¥)
                if not menu_rows and page_num > 0:
                    logger.info(f"   í˜ì´ì§€ {page_num + 1}: ë©”ë‰´ ì—†ìŒ - ì¢…ë£Œ")
                    break
                
                logger.info(f"   í˜ì´ì§€ {page_num + 1}: {len(menu_rows)}ê°œ ë©”ë‰´ ë°œê²¬")
                
                # ì´ í˜ì´ì§€ ì „ì²´ë¥¼ ì €ì¥
                self.stats["total"] += 1
                
                # í’ˆì§ˆ ê²€ì‚¬ (ì²« í˜ì´ì§€ëŠ” ë©”ë‰´ê°€ ì ì–´ë„ ì €ì¥)
                is_quality, reason = self.quality_filter.is_high_quality(html, page_url)
                if not is_quality and page_num > 0:
                    logger.warning(f"   í’ˆì§ˆ í•„í„° ì‹¤íŒ¨: {reason}")
                    self.stats["filtered"] += 1
                    page_num += 1
                    continue
                elif not is_quality and page_num == 0:
                    logger.warning(f"   í’ˆì§ˆ í•„í„° ê²½ê³ : {reason} (ì²« í˜ì´ì§€ì´ë¯€ë¡œ ì €ì¥)")
                    reason = "First page - saved anyway"
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_data = self.content_extractor.extract_with_metadata(html)
                
                # ì‹ë‹¹ ë©”ë‰´ í…Œì´ë¸” ì¶”ì¶œ (íŠ¹ë³„ ì²˜ë¦¬)
                menu_text = self._extract_menu_table(soup)
                if menu_text:
                    # ë©”ë‰´ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ë³¸ë¬¸ì— ì¶”ê°€
                    content_data['text'] = menu_text
                
                # URLì—ì„œ ì‹ë‹¹ ì´ë¦„ ì¶”ì¶œ
                restaurant_name = "ì‹ë‹¹"
                if 'restaurant01' in url:
                    restaurant_name = "í•™ìƒì‹ë‹¹"
                elif 'restaurant02' in url:
                    restaurant_name = "êµì§ì›ì‹ë‹¹"
                elif 'restaurant04' in url:
                    restaurant_name = "ë¶„ì‹ë‹¹"
                elif 'restaurant05' in url:
                    restaurant_name = "ì‹ í‰ìº í¼ìŠ¤ì‹ë‹¹"
                elif 'restaurant_menu01' in url:
                    restaurant_name = "í‘¸ë¦„ê´€"
                elif 'restaurant_menu02' in url:
                    restaurant_name = "ì˜¤ë¦„ê´€1ë™"
                elif 'restaurant_menu03' in url:
                    restaurant_name = "ì˜¤ë¦„ê´€2ë™"
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    "text_length": len(content_data['text']),
                    "word_count": content_data['word_count'],
                    "title": f"{restaurant_name} - í˜ì´ì§€ {page_num + 1}",
                    "paragraphs": content_data['paragraphs'],
                    "page_number": page_num + 1,
                    "menu_count": len(menu_rows),
                    "restaurant_name": restaurant_name,
                    "type": "restaurant_menu",
                    "quality_check": reason,
                    "crawled_at": datetime.now().isoformat(),
                }
                
                # ì €ì¥ (ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì „ë‹¬)
                filepath = self.storage.save_page(
                    page_url, 
                    html, 
                    metadata,
                    extracted_text=content_data['text'],  # ë©”ë‰´ í¬í•¨ëœ í…ìŠ¤íŠ¸
                    title=metadata['title']
                )
                
                self.saved_pages.append({
                    "url": page_url,
                    "file": filepath,
                    "title": metadata['title'],
                    "text_length": len(content_data['text']),
                    "page_number": page_num + 1,
                    "menu_count": len(menu_rows),
                    "restaurant_name": restaurant_name,
                })
                
                self.stats["success"] += 1
                
                logger.info(f"   âœ… ì €ì¥ ì™„ë£Œ: {Path(filepath).name}")
                logger.info(f"      ì‹ë‹¹: {restaurant_name}")
                logger.info(f"      ë©”ë‰´ ê°œìˆ˜: {len(menu_rows)}")
                logger.info(f"      ë³¸ë¬¸ ê¸¸ì´: {len(content_data['text'])} ë¬¸ì")
                
                # ë©”íƒ€ ì •ë³´ ì—…ë°ì´íŠ¸ (í¬ë¡¤ë§ ë‚ ì§œ ì €ì¥)
                restaurant_key = url.split('/')[-1]  # restaurant01.do ë“±
                self.index_meta[f'{restaurant_key}_last_crawl'] = datetime.now().isoformat()
                
                # ì²« í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•˜ë¯€ë¡œ ì¢…ë£Œ
                break
                
            except Exception as e:
                logger.error(f"âŒ ì—ëŸ¬: {e}")
                break
        
        logger.info(f"\nâœ… ì‹ë‹¹ ë©”ë‰´ í¬ë¡¤ë§ ì™„ë£Œ")
    
    def _extract_menu_table(self, soup: BeautifulSoup) -> str:
        """
        <table>ì˜ ê°€ë¡œ(ì—´=ìš”ì¼) / ì„¸ë¡œ(í–‰=ì‹ì‚¬íƒ€ì…) êµ¬ì¡°ë§Œ ì´ìš©í•´ì„œ
        ìš”ì¼ë³„ë¡œ ì¡°ì‹/ì¤‘ì‹/ì„ì‹ ë©”ë‰´ë¥¼ ì •ë¦¬í•´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.

        ì˜ˆì‹œ ì¶œë ¥:
        [ì›”(11.24)]
          ì¤‘ì‹: ë©”ë‰´1 / ë©”ë‰´2 / ...
          ì„ì‹: ë©”ë‰´1 / ë©”ë‰´2 / ...

        [í™”(11.25)]
          ì¤‘ì‹: ...
          ì„ì‹: ...
        """

        # 1) ë©”ë‰´ í…Œì´ë¸” ì°¾ê¸° (captionì— 'ì‹ë‹¹ ë©”ë‰´ í‘œ' ë“¤ì–´ê°„ ê²ƒ ìš°ì„ )
        table = None
        for t in soup.find_all("table"):
            cap = t.find("caption")
            if cap and "ì‹ë‹¹ ë©”ë‰´ í‘œ" in cap.get_text(strip=True):
                table = t
                break
        if table is None:
            table = soup.find("table")
        if table is None:
            return ""

        # 2) í—¤ë”ì—ì„œ ìš”ì¼ ë¼ë²¨ ì¶”ì¶œ (ì—´ ê°œìˆ˜ = ìš”ì¼ ê°œìˆ˜)
        thead = table.find("thead")
        if not thead:
            return ""

        ths = thead.find_all("th")
        day_labels = [th.get_text(" ", strip=True) for th in ths if th.get_text(strip=True)]
        num_days = len(day_labels)
        if num_days == 0:
            return ""

        # per_day[day_index] = { "ì¤‘ì‹": [..ë©”ë‰´..], "ì„ì‹": [..ë©”ë‰´..], ... }
        per_day: list[dict[str, list[str]]] = [dict() for _ in range(num_days)]
        # ì „ì²´ ì‹ì‚¬íƒ€ì… ì¶œë ¥ ìˆœì„œ ìœ ì§€ìš© (ì¡°ì‹ â†’ ì¤‘ì‹ â†’ ì„ì‹ ìˆœ ë“±)
        meal_order: list[str] = []

        # 3) tbodyì˜ ê° í–‰(tr)ì„ ëŒë©´ì„œ, ì…€(td)ì„ ìš”ì¼ ì¸ë±ìŠ¤ì— ë§¤í•‘
        tbody = table.find("tbody")
        if not tbody:
            return ""

        for row in tbody.find_all("tr"):
            tds = row.find_all("td")
            if not tds:
                continue

            # ê° td = í•´ë‹¹ ìš”ì¼ì˜ í•œ ë¼(ì¤‘ì‹/ì„ì‹ ë“±)
            for col_idx, td in enumerate(tds):
                if col_idx >= num_days:
                    break

                p = td.find("p")
                if not p:
                    continue

                meal_name = p.get_text(strip=True)  # ì˜ˆ: "ì¤‘ì‹", "ì„ì‹"
                if not meal_name:
                    continue

                # li í•­ëª©ë“¤ = ì‹¤ì œ ë©”ë‰´ë“¤
                items = [li.get_text(strip=True) for li in td.find_all("li")]
                # liê°€ ì—†ê³  ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš° ëŒ€ì‘í•˜ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì— ì¶”ê°€ ì²˜ë¦¬ ê°€ëŠ¥
                if not items:
                    # td ì•ˆì˜ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ p í…ìŠ¤íŠ¸ëŠ” ë¹¼ê³  ë‚˜ë¨¸ì§€ë¥¼ ë³¼ ìˆ˜ë„ ìˆìŒ
                    # ì—¬ê¸°ì„œëŠ” li ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    continue

                # ì‹ì‚¬íƒ€ì… ë“±ì¥ ìˆœì„œ ê¸°ë¡ (ì¡°ì‹â†’ì¤‘ì‹â†’ì„ì‹ ìˆœì„œ ìœ ì§€)
                if meal_name not in meal_order:
                    meal_order.append(meal_name)

                day_meals = per_day[col_idx]
                if meal_name not in day_meals:
                    day_meals[meal_name] = []
                day_meals[meal_name].extend(items)

        # 4) ìµœì¢… í…ìŠ¤íŠ¸ ì¡°ë¦½: ìš”ì¼ë³„ ë¸”ë¡
        lines: list[str] = []
        for day_idx, day_label in enumerate(day_labels):
            lines.append(f"[{day_label}]")

            day_meals = per_day[day_idx]

            for meal_name in meal_order:
                if meal_name in day_meals and day_meals[meal_name]:
                    menu_str = " / ".join(day_meals[meal_name])
                    lines.append(f"  {meal_name}: {menu_str}")

            lines.append("")  # ìš”ì¼ ì‚¬ì´ ê³µë°±

        return "\n".join(lines).strip()



    
    def run(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print("="*80)
        print("í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*80)
        print(f"ëŒ€ìƒ ì„¹ì…˜:")
        print(f"  1. í•™ì‚¬ì¼ì • (schedule_reg.do)")
        print(f"  2. êµë‚´ ì‹ë‹¹ (restaurant01~05.do)")
        print(f"  3. ìƒí™œê´€ ì‹ë‹¹ (dorm/restaurant_menu01~03.do)")
        print(f"  4. í†µí•™ë²„ìŠ¤ ê³µì§€ ê²Œì‹œíŒ (bus.kumoh.ac.kr)")
        print("="*80)
        
        start_time = datetime.now()
        
        # 1. ë‹¨ì¼ í˜ì´ì§€ URL í¬ë¡¤ë§ (í•™ì‚¬ì¼ì •, ì‹ë‹¹ ë©”ë‰´)
        for url in self.target_urls:
            print(f"\nğŸ“ ëŒ€ìƒ URL: {url}")
            print("-"*80)
            
            # í•™ì‚¬ì¼ì • í˜ì´ì§€ëŠ” ë‚ ì§œ í•„í„° ìŠ¤í‚µ
            skip_date = 'schedule_reg' in url
            
            # í•™ì‚¬ì¼ì •ê³¼ ì‹ë‹¹ ë©”ë‰´ëŠ” ëª¨ë“  í˜ì´ì§€ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§
            if 'schedule_reg' in url:
                logger.info("\n   ğŸ“‹ í•™ì‚¬ì¼ì • - ëª¨ë“  í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
                self.crawl_schedule_lists(url)
            elif 'restaurant' in url:
                logger.info("\n   ğŸ½ï¸ ì‹ë‹¹ ë©”ë‰´ - ëª¨ë“  í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
                self.crawl_restaurant_lists(url)
            
            import time
            time.sleep(1)
        
        # 2. ê²Œì‹œíŒ URL í¬ë¡¤ë§ (í†µí•™ë²„ìŠ¤ ê³µì§€ ë“±)
        for board in self.board_urls:
            url = board["url"]
            name = board["name"]
            max_pages = board.get("max_pages", 5)
            skip_date_filter = board.get("skip_date_filter", False)
            
            print(f"\nğŸ“ ê²Œì‹œíŒ: {name}")
            print(f"   URL: {url}")
            print(f"   ìµœëŒ€ í˜ì´ì§€: {max_pages}")
            print("-"*80)
            
            logger.info(f"\n   ğŸ“‹ [{name}] ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘")
            self.crawl_list_page(
                url, 
                max_pages=max_pages, 
                skip_date_filter=skip_date_filter,
                board_name=name
            )
            
            import time
            time.sleep(1)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        if self.saved_pages:
            # ë©”íƒ€ ì •ë³´ í¬í•¨í•´ì„œ ì €ì¥
            index_data = {
                "crawl_date": datetime.now().isoformat(),
                "total_pages": len(self.saved_pages),
                "meta": self.index_meta,  # ì²« í•­ëª©, í¬ë¡¤ë§ ë‚ ì§œ ë“±
                "pages": self.saved_pages
            }
            self.storage.save_index(index_data)
            logger.info(f"\nğŸ“š ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(self.saved_pages)} í˜ì´ì§€")
        
        # ìµœì¢… í†µê³„
        elapsed = datetime.now() - start_time
        
        print("\n" + "="*80)
        print("í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*80)
        print(f"ì´ ì‹œë„: {self.stats['total']}")
        print(f"ì„±ê³µ: {self.stats['success']}")
        print(f"ê±´ë„ˆëœ€ (ì´ë¯¸ í¬ë¡¤ë§ë¨): {self.stats['skipped']}")
        print(f"ì‹¤íŒ¨: {self.stats['failed']}")
        print(f"í•„í„°ë¨: {self.stats['filtered']}")
        print(f"  - ë‚ ì§œ í•„í„°(2021 ì´ì „): {self.stats['filtered_date']}")
        print(f"  - í’ˆì§ˆ í•„í„°: {self.stats['filtered'] - self.stats['filtered_date']}")
        print(f"\nğŸ“ ì²¨ë¶€íŒŒì¼:")
        print(f"  - ë°œê²¬ë¨: {self.stats['attachments_found']}ê°œ")
        if self.enable_minio:
            print(f"  - MinIO ì—…ë¡œë“œ ì„±ê³µ: {self.stats['attachments_uploaded']}ê°œ")
        else:
            print(f"  - ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡ (MinIO ë¹„í™œì„±í™”)")
        print(f"\nì†Œìš” ì‹œê°„: {elapsed}")
        print("="*80)
        
        # ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜
        output_dir = Path(__file__).parent.parent / "data" / "test_crawled"
        print(f"\nğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        print(f"   - í˜ì´ì§€: {output_dir}/pages/")
        print(f"   - ì¸ë±ìŠ¤: {output_dir}/crawl_index.json")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í…ŒìŠ¤íŠ¸ í¬ë¡¤ëŸ¬ - ì²¨ë¶€íŒŒì¼ MinIO ì—…ë¡œë“œ ì§€ì›')
    parser.add_argument('--enable-minio', action='store_true',
                        help='ì²¨ë¶€íŒŒì¼ì„ MinIOì— ì—…ë¡œë“œ (ê¸°ë³¸ê°’: ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡)')
    args = parser.parse_args()
    
    crawler = SimpleTestCrawler(enable_minio=args.enable_minio)
    crawler.run()


if __name__ == "__main__":
    main()